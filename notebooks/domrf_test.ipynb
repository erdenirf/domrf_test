{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aEM-LVCl0hA"
      },
      "source": [
        "# Загрузка контента из papers.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mu_2rEd2ZFEQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wQdCTndtZDn5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"papers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Jdj6fzmLZI33",
        "outputId": "ae72b2ba-3987-48b1-b14e-d85caaa11632"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
              "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
              "      <td>In my last article, I introduced the concept o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How to Use ggplot2 in Python</td>\n",
              "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
              "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
              "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1386</th>\n",
              "      <td>Brain: A Mystery</td>\n",
              "      <td>“The most beautiful experience we can have is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>Machine Learning: Lincoln Was Ahead of His Time</td>\n",
              "      <td>Photo by Jp Valery on Unsplash\\n\\nIn the 45th ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>AI and Us — an Opera Experience. In my previou...</td>\n",
              "      <td>EKHO COLLECTIVE: OPERA BEYOND SERIES\\n\\nIn my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389</th>\n",
              "      <td>Digital Skills as a Service (DSaaS)</td>\n",
              "      <td>Have you ever thought about what will be in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1390</th>\n",
              "      <td>Primer on The Importance of Mindful Data Colle...</td>\n",
              "      <td>Outline\\n\\nThere are differences in standards ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1391 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Title  \\\n",
              "0     A Beginner’s Guide to Word Embedding with Gens...   \n",
              "1     Hands-on Graph Neural Networks with PyTorch & ...   \n",
              "2                          How to Use ggplot2 in Python   \n",
              "3     Databricks: How to Save Data Frames as CSV Fil...   \n",
              "4     A Step-by-Step Implementation of Gradient Desc...   \n",
              "...                                                 ...   \n",
              "1386                                   Brain: A Mystery   \n",
              "1387    Machine Learning: Lincoln Was Ahead of His Time   \n",
              "1388  AI and Us — an Opera Experience. In my previou...   \n",
              "1389                Digital Skills as a Service (DSaaS)   \n",
              "1390  Primer on The Importance of Mindful Data Colle...   \n",
              "\n",
              "                                                   Text  \n",
              "0     1. Introduction of Word2vec\\n\\nWord2vec is one...  \n",
              "1     In my last article, I introduced the concept o...  \n",
              "2     Introduction\\n\\nThanks to its strict implement...  \n",
              "3     Photo credit to Mika Baumeister from Unsplash\\...  \n",
              "4     A Step-by-Step Implementation of Gradient Desc...  \n",
              "...                                                 ...  \n",
              "1386  “The most beautiful experience we can have is ...  \n",
              "1387  Photo by Jp Valery on Unsplash\\n\\nIn the 45th ...  \n",
              "1388  EKHO COLLECTIVE: OPERA BEYOND SERIES\\n\\nIn my ...  \n",
              "1389  Have you ever thought about what will be in th...  \n",
              "1390  Outline\\n\\nThere are differences in standards ...  \n",
              "\n",
              "[1391 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa_Lt8ukaP3"
      },
      "source": [
        "# Векторная база данных Qdrant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVzhtC-ckn9i"
      },
      "source": [
        "### Credentials / Секретные коды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SaNdZ-uzZmC6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "OPENAI_PROXY = os.environ.get('OPENAI_PROXY')\n",
        "QDRANT_URL = os.environ.get('QDRANT_URL')\n",
        "QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY')\n",
        "QDRANT_collection_name = os.environ.get('QDRANT_collection_name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'http://localhost:6333'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "QDRANT_URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IKBAaBlk4ts"
      },
      "source": [
        "### Установка библиотек, функций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wEv3n7Bca4_M",
        "outputId": "72061618-00db-4448-87c7-eb6ffdee60d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (3.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.2.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.12)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.52.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.12->langchain-openai) (3.10.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.12->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.12->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.12->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Requirement already satisfied: langchain-qdrant in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain-qdrant) (0.3.12)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain-qdrant) (2.9.2)\n",
            "Requirement already satisfied: qdrant-client<2.0.0,>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from langchain-qdrant) (1.12.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.52->langchain-qdrant) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.52->langchain-qdrant) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.52->langchain-qdrant) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.52->langchain-qdrant) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.52->langchain-qdrant) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.52->langchain-qdrant) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (2.23.4)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.67.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.67.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.26.4)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (2.10.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (2.2.3)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (5.28.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (4.1.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.52->langchain-qdrant) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.1.52->langchain-qdrant) (3.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.1.52->langchain-qdrant) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.1.52->langchain-qdrant) (1.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (4.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.1.52->langchain-qdrant) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.2.2)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-text-splitters) (0.3.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-qdrant\n",
        "!pip install langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CLOAFuwzcUoH"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_qdrant.qdrant import QdrantVectorStore\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nxOyubxDcleM"
      },
      "outputs": [],
      "source": [
        "def loader(titles: list[str], texts: list[str]) -> list[Document]:\n",
        "    \"\"\"Загружает 2 массива одинаковой длины и создает список документов.\n",
        "\n",
        "    Args:\n",
        "        titles (list[str]): Список заголовков\n",
        "        texts (list[str]): Список текстов\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: Список объектов Document\n",
        "    \"\"\"\n",
        "    result: list[Document] = []\n",
        "    for index, title in enumerate(titles):\n",
        "        result.append(Document( page_content = title, metadata = { \"source\": texts[index] }))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cXrI676qjWkF"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
        "    \"\"\"Возвращает количество токенов в текстовой строке.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def docs_num_tokens(docs: list[Document]) -> int:\n",
        "    \"\"\"\n",
        "    Функция вычисляет общее количество токенов в списке документов.\n",
        "\n",
        "    Args:\n",
        "        docs (list[Document]): Список объектов Document, для которых необходимо подсчитать токены.\n",
        "\n",
        "    Returns:\n",
        "        int: Общее количество токенов во всех документах.\n",
        "    \"\"\"\n",
        "    return sum([num_tokens_from_string(doc.page_content) for doc in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T1W4JK0lKN2"
      },
      "source": [
        "### Langchain Pipeline - Индексация векторной базы данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XejynvvEeCsL"
      },
      "outputs": [],
      "source": [
        "docs = loader(df['Title'].values, df['Text'].values)\n",
        "prompt_tokens = docs_num_tokens(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LcZRAtSJeQow",
        "outputId": "2490c7f1-81f7-4ac4-97bc-f7444c37c81b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '1. Introduction of Word2vec\\n\\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\\n\\nThere are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\\n\\n2. Gensim Python Library Introduction\\n\\nGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\\n\\nAt first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\\n\\nPython >= 2.7 (tested with versions 2.7, 3.5 and 3.6)\\n\\n>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3\\n\\n>= 1.11.3 SciPy >= 0.18.1\\n\\n>= 0.18.1 Six >= 1.5.0\\n\\n>= 1.5.0 smart_open >= 1.2.1\\n\\nThere are two ways for installation. We could run the following code in our terminal to install genism package.\\n\\npip install --upgrade gensim\\n\\nOr, alternatively for Conda environments:\\n\\nconda install -c conda-forge gensim\\n\\n3. Implementation of word Embedding with Gensim Word2Vec Model\\n\\nIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\\n\\nThis vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\\n\\n>>> df = pd.read_csv(\\'data.csv\\')\\n\\n>>> df.head()\\n\\n3.1 Data Preprocessing:\\n\\nSince the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\\n\\nGenism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\\n\\nTo achieve this, we need to do the following things :\\n\\na. Create a new column for Make Model\\n\\n>>> df[\\'Maker_Model\\']= df[\\'Make\\']+ \" \" + df[\\'Model\\']\\n\\nb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\\n\\n# Select features from original dataset to form a new dataframe\\n\\n>>> df1 = df[[\\'Engine Fuel Type\\',\\'Transmission Type\\',\\'Driven_Wheels\\',\\'Market Category\\',\\'Vehicle Size\\', \\'Vehicle Style\\', \\'Maker_Model\\']] # For each row, combine all the columns into one column\\n\\n>>> df2 = df1.apply(lambda x: \\',\\'.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\\n\\n>>> df_clean = pd.DataFrame({\\'clean\\': df2}) # Create the list of list format of the custom corpus for gensim modeling\\n\\n>>> sent = [row.split(\\',\\') for row in df_clean[\\'clean\\']] # show the example of list of list format of the custom corpus for gensim modeling\\n\\n>>> sent[:2]\\n\\n[[\\'premium unleaded (required)\\',\\n\\n\\'MANUAL\\',\\n\\n\\'rear wheel drive\\',\\n\\n\\'Factory Tuner\\',\\n\\n\\'Luxury\\',\\n\\n\\'High-Performance\\',\\n\\n\\'Compact\\',\\n\\n\\'Coupe\\',\\n\\n\\'BMW 1 Series M\\'],\\n\\n[\\'premium unleaded (required)\\',\\n\\n\\'MANUAL\\',\\n\\n\\'rear wheel drive\\',\\n\\n\\'Luxury\\',\\n\\n\\'Performance\\',\\n\\n\\'Compact\\',\\n\\n\\'Convertible\\',\\n\\n\\'BMW 1 Series\\']]\\n\\n3.2. Genism word2vec Model Training\\n\\nWe can train the genism word2vec model with our own custom corpus as following:\\n\\n>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\\n\\nLet’s try to understand the hyperparameters of this model.\\n\\nsize: The number of dimensions of the embeddings and the default is 100.\\n\\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.\\n\\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\\n\\nworkers: The number of partitions during training and the default workers is 3.\\n\\nsg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\\n\\nAfter training the word2vec model, we can obtain the word embedding directly from the training model as following.\\n\\n>>> model[\\'Toyota Camry\\'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\\n\\n-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\\n\\n-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\\n\\n0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\\n\\n-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\\n\\n-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\\n\\n-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\\n\\n0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\\n\\n0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,\\n\\n0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\\n\\ndtype=float32)\\n\\n4. Compute Similarities\\n\\nNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\\n\\n>>> model.similarity(\\'Porsche 718 Cayman\\', \\'Nissan Van\\')\\n\\n0.822824584626184 >>> model.similarity(\\'Porsche 718 Cayman\\', \\'Mercedes-Benz SLK-Class\\')\\n\\n0.961089779453727\\n\\nFrom the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\\n\\n>>> model1.most_similar(\\'Mercedes-Benz SLK-Class\\')[:5] [(\\'BMW M4\\', 0.9959905743598938),\\n\\n(\\'Maserati Coupe\\', 0.9949707984924316),\\n\\n(\\'Porsche Cayman\\', 0.9945154190063477),\\n\\n(\\'Mercedes-Benz SLS AMG GT\\', 0.9944609999656677),\\n\\n(\\'Maserati Spyder\\', 0.9942780137062073)]\\n\\nHowever, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\\n\\nThe following function shows how can we generate the most similar make model based on cosine similarity.\\n\\ndef cosine_distance (model, word,target_list , num) :\\n\\ncosine_dict ={}\\n\\nword_list = []\\n\\na = model[word]\\n\\nfor item in target_list :\\n\\nif item != word :\\n\\nb = model [item]\\n\\ncos_sim = dot(a, b)/(norm(a)*norm(b))\\n\\ncosine_dict[item] = cos_sim\\n\\ndist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\\n\\nfor item in dist_sort:\\n\\nword_list.append((item[0], item[1]))\\n\\nreturn word_list[0:num] # only get the unique Maker_Model\\n\\n>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\\n\\n>>> cosine_distance (model,\\'Mercedes-Benz SLK-Class\\',Maker_Model,5) [(\\'Mercedes-Benz CLK-Class\\', 0.99737006),\\n\\n(\\'Aston Martin DB9\\', 0.99593246),\\n\\n(\\'Maserati Spyder\\', 0.99571854),\\n\\n(\\'Ferrari 458 Italia\\', 0.9952333),\\n\\n(\\'Maserati GranTurismo Convertible\\', 0.994994)]\\n\\n5. T-SNE Visualizations\\n\\nIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\\n\\ndef display_closestwords_tsnescatterplot(model, word, size):\\n\\n\\n\\narr = np.empty((0,size), dtype=\\'f\\')\\n\\nword_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\\n\\nfor wrd_score in close_words:\\n\\nwrd_vector = model[wrd_score[0]]\\n\\nword_labels.append(wrd_score[0])\\n\\narr = np.append(arr, np.array([wrd_vector]), axis=0)\\n\\n\\n\\ntsne = TSNE(n_components=2, random_state=0)\\n\\nnp.set_printoptions(suppress=True)\\n\\nY = tsne.fit_transform(arr) x_coords = Y[:, 0]\\n\\ny_coords = Y[:, 1]\\n\\nplt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\\n\\nplt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\\'offset points\\')\\n\\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\\n\\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\\n\\nplt.show() >>> display_closestwords_tsnescatterplot(model, \\'Porsche 718 Cayman\\', 50)\\n\\nThis T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\\n\\nAbout Me\\n\\nI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.'}, page_content='A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4pESw0Rj0Ts",
        "outputId": "baaec1df-d2d3-4d8d-b50a-40a900a51261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество токенов docs = 14142\n"
          ]
        }
      ],
      "source": [
        "print(f\"Количество токенов docs = {prompt_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QnvvEINme_gp"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions = 256, api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yBCSLOcCgw3N"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
            "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
          ]
        }
      ],
      "source": [
        "qdrant = QdrantVectorStore.from_documents(docs,\n",
        "                                          embeddings,\n",
        "                                          collection_name=QDRANT_collection_name,\n",
        "                                          api_key=QDRANT_API_KEY,\n",
        "                                          url=QDRANT_URL,\n",
        "                                          force_recreate=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBu1GCyPmL4H"
      },
      "source": [
        "### Cosine Similarity - Семантический поиск. 4 примера"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aygt4oA7Dyn7"
      },
      "outputs": [],
      "source": [
        "def docs_related(query: str, top_k: int = 4):\n",
        "    return qdrant.similarity_search(query, k = top_k)\n",
        "\n",
        "def keys_related(docs: list[Document]) -> str:\n",
        "    return \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "def values_related(docs: list[Document]) -> str:\n",
        "    return \"\\n\\n---\\n\\n\".join([doc.metadata.get(\"source\") for doc in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xD9F1otyG0qy"
      },
      "outputs": [],
      "source": [
        "Questions = [\n",
        "    \"Where can I apply Convolutional Neural Network?\",\n",
        "    \"What is Reinforcement Learning?\",\n",
        "    \"How to deploy a machine learning model?\",\n",
        "    \"How to implement a random forest algorithm?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wQmPszw5HIdj"
      },
      "outputs": [],
      "source": [
        "Docs = list(map(docs_related, Questions))\n",
        "Keys = list(map(keys_related, Docs))\n",
        "Values = list(map(values_related, Docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nBVYryGHcxf",
        "outputId": "e0cc85bc-fe30-4d1a-8f5b-c2d0ebadd41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Convolutional Neural Networks\n",
            "An introduction to Convolutional Neural Networks\n",
            "CNN vs fully-connected network for image processing\n",
            "Understanding Neural Networks\n"
          ]
        }
      ],
      "source": [
        "print(Keys[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xAgsO32AGC0D",
        "outputId": "0c1cf6e6-7b66-40ad-a68d-107b454cdc3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Researchers came up with the concept of CNN or Convolutional Neural Network while working on image processing algorithms. Traditional fully connected networks were kind of a black box — that took in all of the inputs and passed through each value to a dense network that followed into a one hot output. That seemed to work with small set of inputs.\n",
            "\n",
            "But, when we work on a image of 1024x768 pixels, we have an input of 3x1024x768 = 2359296 numbers (RGB values per pixel). A dense multi layer neural network that consumes an input vector of 2359296 numbers would have at least 2359296 weights per neuron in the first layer itself — 2MB of weights per neuron of the first layer. That would be crazy! For the processor as well as the RAM. Back in 1990’s and early 2000’s, this was almost impossible.\n",
            "\n",
            "That led researchers wondering if there is a better way of doing this job. The first and foremost task in any image processing (recognition or manipulation) is typically detecting the edges and texture. This is followed by identifying and working on the real objects. If we agree on this, it is obvious to note that detecting the texture and edges really does not depend on the entire image. One needs to look at the pixels around a given pixel to identify an edge or a texture.\n",
            "\n",
            "Moreover, the algorithm (whatever it is), for identifying edges or the texture should be the same across the image. We cannot have a different algorithm for the center of the image or any corner or side. The concept of detecting edge or texture has to be the same. We don’t need to learn a new set of parameters for every pixel of the image.\n",
            "\n",
            "This understanding led to the convolutional neural networks. The first layer of the network is made of small chunk of neurons that scan across the image — processing a few pixels at a time. Typically these are squares of 9 or 16 or 25 pixels.\n",
            "\n",
            "CNN reduces the computation very efficiently. The small “filter/kernel” slides along the image, working on small blocks at a time. The processing required across the image is quite similar and hence this works very well. If you are interested in a detailed study of the subject, check out this paper by Matthew D. Zeiler and Rob Fergus\n",
            "\n",
            "Although it was introduced for image processing, over the years, CNN has found application in many other domains.\n",
            "\n",
            "An Example\n",
            "\n",
            "---\n",
            "\n",
            "An introduction to Convolutional Neural Networks\n",
            "\n",
            "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data.\n",
            "\n",
            "A convolution is essentially sliding a filter over the input. One helpful way to think about convolutions is this quote from Dr Prasad Samarakoon: “A convolution can be thought as “looking at a function’s surroundings to make better/accurate predictions of its outcome.”\n",
            "\n",
            "Rather than looking at an entire image at once to find certain features it can be more effective to look at smaller portions of the image.\n",
            "\n",
            "Common uses for CNNs\n",
            "\n",
            "The most common use for CNNs is image classification, for example identifying satellite images that contain roads or classifying hand written letters and digits. There are other quite mainstream tasks such as image segmentation and signal processing, for which CNNs perform well at.\n",
            "\n",
            "CNNs have been used for understanding in Natural Language Processing (NLP) and speech recognition, although often for NLP Recurrent Neural Nets (RNNs) are used.\n",
            "\n",
            "A CNN can also be implemented as a U-Net architecture, which are essentially two almost mirrored CNNs resulting in a CNN whose architecture can be presented in a U shape. U-nets are used where the output needs to be of similar size to the input such as segmentation and image improvement.\n",
            "\n",
            "Interesting uses for CNNs other than image processing\n",
            "\n",
            "More and more diverse and interesting uses are being found for CNN architectures. An example of a non-image based application is “The Unreasonable Effectiveness of Convolutional Neural Networks in Population Genetic Inference” by Lex Flagel et al. This is used to perform selective sweeps, finding gene flow, inferring population size changes, inferring rate of recombination.\n",
            "\n",
            "There are researchers such as Professor Gerald Quon at the Quon-titative biology lab, using CNNs for generative models in single cell genomics for disease identification.\n",
            "\n",
            "CNNs are also being used in astrophysics to interpret radio telescope data to predict the likely visual image to represent the data.\n",
            "\n",
            "Deepmind’s WaveNet is a CNN model for generating synthesized voice, used as the basis for Google’s Assistant’s voice synthesizer.\n",
            "\n",
            "Convolutional kernels\n",
            "\n",
            "Each convolutional layer contains a series of filters known as convolutional kernels. The filter is a matrix of integers that are used on a subset of the input pixel values, the same size as the kernel. Each pixel is multiplied by the corresponding value in the kernel, then the result is summed up for a single value for simplicity representing a grid cell, like a pixel, in the output channel/feature map.\n",
            "\n",
            "These are linear transformations, each convolution is a type of affine function.\n",
            "\n",
            "In computer vision the input is often a 3 channel RGB image. For simplicity, if we take a greyscale image that has one channel (a two dimensional matrix) and a 3x3 convolutional kernel (a two dimensional matrix). The kernel strides over the input matrix of numbers moving horizontally column by column, sliding/scanning over the first rows in the matrix containing the images pixel values. Then the kernel strides down vertically to subsequent rows. Note, the filter may stride over one or several pixels at a time, this is detailed further below.\n",
            "\n",
            "In other non-vision applications, a one dimensional convolution may slide vertically over an input matrix.\n",
            "\n",
            "Creating a feature map from a convolutional kernel\n",
            "\n",
            "Below is a diagram showing the operation of the convolutional kernel.\n",
            "\n",
            "A stride one 3x3 convolutional kernel acting on a 8x8 input image, outputting an 8x8 filter/channel. Source: https://www.researchgate.net/figure/a-Illustration-of-the-operation-principle-of-the-convolution-kernel-convolutional-layer_fig2_309487032\n",
            "\n",
            "Below is a visualisation from an excellent presentation, showing the kernel scanning over the values in the input matrix.\n",
            "\n",
            "Kernel scanning over the values in the input matrix. Source: Otavio Good: excerpt https://www.youtube.com/watch?v=f0t-OCG79-U from https://www.youtube.com/watch?v=Oqm9vsf_hvU\n",
            "\n",
            "Padding\n",
            "\n",
            "To handle the edge pixels there are several approaches:\n",
            "\n",
            "Losing the edge pixels\n",
            "\n",
            "Padding with zero value pixels\n",
            "\n",
            "Reflection padding\n",
            "\n",
            "Reflection padding is by far the best approach, where the number of pixels needed for the convolutional kernel to process the edge pixels are added onto the outside copying the pixels from the edge of the image. For a 3x3 kernel, one pixel needs to be added around the outside, for a 7x7 kernel then three pixels would be reflected around the outside. The pixels added around each side is the dimension, halved and rounded down.\n",
            "\n",
            "Traditionally in many research papers, the edge pixels are just ignored, which loses a small proportion of the data and this gets increasing worse if there are many deep convolutional layers. For this reason, I could not find existing diagrams to easily convey some of the points here without being misleading and confusing stride 1 convolutions with stride 2 convolutions.\n",
            "\n",
            "With padding, the output from a input of width w and height h would be width w and height h (the same as the input with a single input channel), assuming the kernel takes a stride of one pixel at a time.\n",
            "\n",
            "Creating multiple channels/feature maps with multiple kernels\n",
            "\n",
            "When multiple convolutional kernels are applied within a convolutional layer, many channels/feature maps are created, one from each convolutional kernel. Below is a visualisation below showing the channels/feature maps being created.\n",
            "\n",
            "Visualisation of channels/feature maps created from a layer of convolutional kernels. Source: Otavio Good: excerpt https://www.youtube.com/watch?v=f0t-OCG79-U from https://www.youtube.com/watch?v=Oqm9vsf_hvU\n",
            "\n",
            "RGB 3 channel input\n",
            "\n",
            "Most image processing needs to operate on RGB images with three channels. A RGB image is a three dimensional array of numbers otherwise known as a rank three tensor.\n",
            "\n",
            "When processing a three channel RGB image, a convolutional kernel that is a three dimensional array/rank 3 tensor of numbers would normally be used. It is very common for the convolutional kernel to be of size 3x3x3 — the convolutional kernel being like a cube.\n",
            "\n",
            "Usually there is at least three convolutional kernels in order that each can act as a different filter to gain insight from each colour channel.\n",
            "\n",
            "The convolution kernels as a group make a four dimensional array, otherwise known as a rank four tensor. It is difficult, if not impossible, to visualise dimensions when they are higher than three. In this case imagine it as a list of three dimensional cubes.\n",
            "\n",
            "The filter moves across the input data in the same way, sliding or taking strides across the rows then moving down the columns and striding across the rows until it reaches the bottom right corner:\n",
            "\n",
            "3x3x3 convolutional kernel acting on a 3 channel input. Source: https://machinethink.net/images/vggnet-convolutional-neural-network-iphone/ConvolutionKernel@2x.png\n",
            "\n",
            "With padding and a stride of one, the output from an input of width x, height y and depth 3 would be width x, height y and depth 1, as the cube produces a single summed output value from each stride. For example, with an input of 3x64x64 (say a 64x64 RGB three channel image) then one kernel taking strides of one with padding the edge pixels would output a channel/feature map of 64x64 (one channel).\n",
            "\n",
            "It is worth noting the input is often normalised, this is detailed further below.\n",
            "\n",
            "Strides\n",
            "\n",
            "It is common to use a stride two convolution rather than a stride one convolution, where the convolutional kernel strides over 2 pixels at a time, for example our 3x3 kernel would start at position (1,1), then stride to (1,3), then to 1, 5) and so on, halving the size of the output channel/feature map, compared to the convolutional kernel taking strides of one.\n",
            "\n",
            "With padding, the output from an input of width w, height h and depth 3 would be the ceiling of width w/2, height h/2 and depth 1, as the kernel outputs a single summed output from each stride.\n",
            "\n",
            "For example, with an input of 3x64x64 (say a 64x64 RGB three channel image), one kernel taking strides of two with padding the edge pixels, would produce a channel/feature map of 32x32.\n",
            "\n",
            "Many kernels\n",
            "\n",
            "In CNN models there are often there are many more than three convolutional kernels, 16 kernels or even 64 kernels in a convolutional layer is common.\n",
            "\n",
            "These different convolution kernels each act as a different filter creating a channel/feature map representing something different. For example, kernels could be filtering top edges, bottom edges, diagonal lines and so on. In much deeper networks these kernels could be filtering to animal features such as eyes or bird wings.\n",
            "\n",
            "Having a higher number of convolutional kernels creates a higher number of channels/feature maps and a growing amount of data and this uses more memory. The stride 2 convolution, as per the above example, helps to reduce the memory usage as the output channel of the stride 2 convolution has half the width and height of the input. This assumes reflection padding is being used otherwise it could be slightly smaller.\n",
            "\n",
            "An example of several convolutional layers of stride 2\n",
            "\n",
            "With a 64 pixel square input with three channels and 16 3x3x3 kernels our convolutional layer would have:\n",
            "\n",
            "Input: 64x64x3\n",
            "\n",
            "Convolutional kernels: 16x3x3x3 (a four dimensional tensor)\n",
            "\n",
            "Output/activations of the convolutional kernels: 16x32x32 (16 channels/feature maps of 32x32)\n",
            "\n",
            "The network could then apply batch normalisation to decrease learning time and reduce overfitting, more details below. In addition a non-linear activation function, such as RELU is usually applied to allow the network to approximate better, more details below.\n",
            "\n",
            "Often there are several layers of stride 2 convolutions, creating an increasing number of channels/feature maps. Taking the example above a layer deeper:\n",
            "\n",
            "Input: 16x32x32\n",
            "\n",
            "Convolutional kernels: 64x3x3x3\n",
            "\n",
            "Output/activations of the convolutional kernels: 64x16x16 (64 channels/feature maps of 16x16)\n",
            "\n",
            "Then after applying ReLU and batch normalisation (see below), another stride 2 convolution is applied:\n",
            "\n",
            "Input: 64x16x16\n",
            "\n",
            "Convolutional kernels: 128x3x3x3\n",
            "\n",
            "Output/activations of the convolutional kernels: 128x8x8 (128 channels/feature maps of 8x8).\n",
            "\n",
            "Classification\n",
            "\n",
            "If, for example, an image belongs to one of 42 categories and the network’s goal is to predict which category the image belongs to.\n",
            "\n",
            "Following on from the above example with an output of 128x8x8, first the average pool of the rank 3 tensor is taken. The average pool is the mean average of each channel, in the this example each 8x8 matrix is averaged into a single number, with 128 channels/feature maps. This creates 128 numbers, a vector of size 1x128.\n",
            "\n",
            "The next layer is a matrix or rank 2 tensor of 128x42 weights. The input 1x128 matrix is (dot product) multiplied by the 128x42 matrix producing a 1x42 vector. How activated each of the 42 grid cells/vector elements are, is how much the prediction matches that classification represented by that vector element. Softmax is applied as an activation function and then argmax to select the element highest value.\n",
            "\n",
            "Rectified Linear Unit (ReLU)\n",
            "\n",
            "A Rectified Linear Unit is used as a non-linear activation function. A ReLU says if the value is less than zero, round it up to zero.\n",
            "\n",
            "Normalisation\n",
            "\n",
            "Normalisation is the process of subtracting the mean and dividing by the standard deviation. It transforms the range of the data to be between -1 and 1 making the data use the same scale, sometimes called Min-Max scaling.\n",
            "\n",
            "It is common to normalize the input features, standardising the data by removing the mean and scaling to unit variance. It is often important the input features are centred around zero and have variance in the same order.\n",
            "\n",
            "With some data, such as images the data is scaled so that it’s range is between 0 and 1, most simply dividing the pixel values by 255.\n",
            "\n",
            "This also allows the training process to find the optimal parameters quicker.\n",
            "\n",
            "Batch normalisation\n",
            "\n",
            "Batch normalisation has the benefits of helping to make a network output more stable predictions, reduce overfitting through regularisation and speeds up training by an order of magnitude.\n",
            "\n",
            "Batch normalisation is the process of carrying normalisation within the scope activation layer of the current batch, subtracting the mean of the batch’s activations and dividing by the standard deviation of the batches activations.\n",
            "\n",
            "This is necessary as even after normalizing the input as some activations can be higher, which can cause the subsequent layers to act abnormally and makes the network less stable.\n",
            "\n",
            "As batch normalisation has scaled and shifted the activation outputs, the weights in the next layer will no longer be optimal. Stochastic gradient descent (SGD) would undo the normalisation, as it would minimise the loss function.\n",
            "\n",
            "To prevent this effect two trainable parameters can be added to each layer to allow SGD to denormalise the output. These parameters are a mean parameter “beta” and a standard deviation parameter “gamma”. Batch normalisation sets these two weights for each activation output to allow the normalisation to be reversed to get the raw input, this avoids affecting the stability of the network by avoiding having to update the other weights.\n",
            "\n",
            "Why CNNs are so powerful\n",
            "\n",
            "In simple terms a large enough CNN can solve any solvable problem.\n",
            "\n",
            "Notable CNN architecture’s that perform exceptionally well across many different image processing tasks are the VGG models ( K. Simonyan and A. Zisserman), the ResNet models (Kaiming He et al) and the Google Inception models (Christian Szegedy et al). These models have millions of trainable parameters.\n",
            "\n",
            "Universal approximation theorem\n",
            "\n",
            "The Universal approximation theorem essentially states if a problem can be solved it can be solved by deep neural networks, given enough layers of affine functions layered with non-linear functions. Essentially a stack of linear functions followed by non-linear functions could solve any problem that is solvable.\n",
            "\n",
            "Practically in implementation this can be many matrix multiplication with large enough matrices followed by RELU, stacked together these have a mathematical property resulting in being able to solve any arbitrary complex mathematical function to any arbitrary high level of accuracy assuming you have the time and resource to train it.\n",
            "\n",
            "Whether this would give the neural network understanding is a debated topic, especially by cognitive scientists. The argument is that no matter how well you approximate the syntax and semantics of a problem, you never understand it. This is basically the foundation of Searle’s Chinese Room Argument. Some would argue that does it matter if you can approximate the solution to the problem well enough that it’s indistinguishable from understanding the problem.\n",
            "\n",
            "Fastai courses\n",
            "\n",
            "I would like to thank the Fastai team whose courses have helped cement my deep learning and CNN knowledge providing an excellent starting point for further learning and understanding.\n",
            "\n",
            "---\n",
            "\n",
            "Introduction\n",
            "\n",
            "The objective of this article is to provide a theoretical perspective to understand why (single layer) CNNs work better than fully-connected networks for image processing. Linear algebra (matrix multiplication, eigenvalues and/or PCA) and a property of sigmoid/tanh function will be used in an attempt to have a one-to-one (almost) comparison between a fully-connected network (logistic regression) and CNN. Finally, the tradeoff between filter size and the amount of information retained in the filtered image will be examined for the purpose of prediction. For simplicity, we will assume the following:\n",
            "\n",
            "The fully-connected network does not have a hidden layer (logistic regression) Original image was normalized to have pixel values between 0 and 1 or scaled to have mean = 0 and variance = 1 Sigmoid/tanh activation is used between input and convolved image, although the argument works for other non-linear activation functions such as ReLU. ReLU is avoided because it breaks the rigor of the analysis if the images are scaled (mean = 0, variance = 1) instead of normalized Number of channels = depth of image = 1 for most of the article, model with higher number of channels will be discussed briefly The problem involves a classification task. Therefore, C > 1 There are no non-linearities other than the activation and no non-differentiability (like pooling, strides other than 1, padding, etc.) Negative log likelihood loss function is used to train both networks\n",
            "\n",
            "Symbols and Notation\n",
            "\n",
            "Symbols used are:\n",
            "\n",
            "x: Matrix of 2-D input images W₁, b₁: Weight matrix and bias term used for mapping\n",
            "\n",
            "Raw image to output in fully-connected network\n",
            "\n",
            "Filtered image to output in CNN p: Output probability X₁: Filtered image x₁: Filtered-activated image\n",
            "\n",
            "Two conventions to note about the notation are:\n",
            "\n",
            "Dimensions are written between {} Different dimensions are separated by x. Eg: {n x C} represents two dimensional ‘array’\n",
            "\n",
            "Model definition\n",
            "\n",
            "Fully-connected network\n",
            "\n",
            "FC1: Pre-ouptut layer\n",
            "\n",
            "FC2: Estimated probability\n",
            "\n",
            "Convolution neural network\n",
            "\n",
            "C1: Filtered image\n",
            "\n",
            "C2: Filtered-activated image\n",
            "\n",
            "Activation functions\n",
            "\n",
            "C3: Pre-output layer\n",
            "\n",
            "C4: Estimated probability\n",
            "\n",
            "The Mathematics\n",
            "\n",
            "Reducing the CNN to a fully-connected network\n",
            "\n",
            "Let us assume that the filter is square with kₓ = 1 and K(a, b) = 1. Therefore, X₁ = x. Now the advantage of normalizing x and a handy property of sigmoid/tanh will be used. It is discussed below:\n",
            "\n",
            "Required property of sigmoid/tanh\n",
            "\n",
            "Sigmoid activation as a function of input. Courtesy: ResearchGate article [1]\n",
            "\n",
            "We observe that the function is linear for input is small in magnitude. Since the input image was normalized or scaled, all values x will lie in a small region around 0 such that |x| < ϵ for some non-zero ϵ. Therefore, for some constant k and for any point X(a, b) on the image:\n",
            "\n",
            "This suggests that the amount of information in the filtered-activated image is very close to the amount of information in the original image. All the pixels of the filtered-activated image are connected to the output layer (fully-connected).\n",
            "\n",
            "Let us assumed that we learnt optimal weights W₁, b₁ for a fully-connected network with the input layer fully connected to the output layer. We can directly obtain the weights for the given CNN as W₁(CNN) = W₁/k rearranged into a matrix and b₁(CNN) = b₁. Therefore, for a square filter with kₓ = 1 and K(1, 1) = 1 the fully-connected network and CNN will perform (almost) identically.\n",
            "\n",
            "Since tanh is a rescaled sigmoid function, it can be argued that the same property applies to tanh. This can also be observed in the plot below:\n",
            "\n",
            "tanh activation as a function of input. Courtesy: Wolfram MathWorld [2]\n",
            "\n",
            "Filter — worst-case scenario\n",
            "\n",
            "Let us consider a square filter on a square image with kₓ = nₓ, and K(a, b) = 1 for all a, b. Firstly, this filter maps each image to one value (filtered image), which is then mapped to C outputs. Therefore, the filtered image contains less information (information bottleneck) than the output layer — any filtered image with less than C pixels will be the bottleneck. Secondly, this filter maps each image into a single pixel equal to the sum of values of the image. This clearly contains very little information about the original image. Let us consider MNIST example to understand why: consider images with true labels ‘2’ and ‘5’. Sum of values of these images will not differ by much, yet the network should learn a clear boundary using this information.\n",
            "\n",
            "Relaxing the worst-case part 1: filter weights\n",
            "\n",
            "Let us consider a square filter on a square image with kₓ = nₓ but not all values are equal in K. This allows variation in K such that importance is to give to certain pixels or regions (setting all other weights to constant and varying only these weights). By varying K we may be able to discover regions of the image that help in separating the classes. For example — in MNIST, assuming hypothetically that all digits are centered and well-written as per a common template, this may create reasonable separation between the classes even though only 1 value is mapped to C outputs. Consider this case to be similar to discriminant analysis, where a single value (discriminant function) can separate two or more classes.\n",
            "\n",
            "Relaxing the worst-case part 2: filter width\n",
            "\n",
            "Let us consider a square filter on a square image with K(a, b) = 1 for all a, b, but kₓ ≠ nₓ. For example, let us consider kₓ = nₓ-1. The original and filtered image are shown below:\n",
            "\n",
            "Original image\n",
            "\n",
            "Filtered image\n",
            "\n",
            "Notice that the filtered image summations contain elements in the first row, first column, last row and last column only once. All other elements appear twice. Assuming the values in the filtered image are small because the original image was normalized or scaled, the activated filtered image can be approximated as k times the filtered image for a small value k. Under linear operations such as matrix multiplication (with weight matrix), the amount of information in k*x₁ is same as the amount of information in x₁ when k is non-zero (true here since the slope of sigmoid/tanh is non-zero near the origin). Therefore, the filtered-activated image contains (approximately) the same amount of information as the filtered image (very loosely written for ease of understanding, because [Fisher] ‘information’ is the variance of the score function, which is related to the variance of the RV. A better version of this statement is: “the scaled/normalized input image and scaled/normalized filtered will have approximately the same amount of information”).\n",
            "\n",
            "Assuming the original image has non-redundant pixels and non-redundant arrangement of pixels, the column space of the image reduced from (nₓ, nₓ) to (2, 2) on application of (nₓ-1, nₓ-1) filter. This causes loss of information, but it is guaranteed to retain more information than (nₓ, nₓ) filter for K(a, b) = 1. As the filter width decreases, the amount of information retained in the filtered (and therefore, filtered-activated) image increases. It reaches the maximum value for kₓ = 1.\n",
            "\n",
            "In a practical case such as MNIST, most of the pixels near the edges are redundant. Therefore, almost all the information can be retained by applying a filter of size ~ width of patch close to the edge with no digit information.\n",
            "\n",
            "Putting things together\n",
            "\n",
            "A peculiar property of CNN is that the same filter is applied at all regions of the image. This is called weight-sharing. The total number of parameters in the model = (kₓ * kₓ) + (nₓ-kₓ+1)*(nₓ-kₓ+1)*C.\n",
            "\n",
            "Larger filter leads to smaller filtered-activated image, which leads to smaller amount of information passed through the fully-connected layer to the output layer. This leads to low signal-to-noise ratio, higher bias, but reduces the overfitting because the number of parameters in the fully-connected layer is reduced. This is a case of high bias, low variance. Smaller filter leads to larger filtered-activated image, which leads to larger amount of information passed through the fully-connected layer to the output layer. This leads to high signal-to-noise ratio, lower bias, but may cause overfitting because the number of parameters in the fully-connected layer is increased. This is a case of low bias, high variance.\n",
            "\n",
            "It is known that K(a, b) = 1 and kₓ=1 performs (almost) as well as a fully-connected network. By adjusting K(a, b) for kₓ ≠ 1 through backpropagation (chain rule) and SGD, the model is guaranteed to perform better on the training set. It also tends to have a better bias-variance characteristic than a fully-connected network when trained with a different set of hyperparameters (kₓ).\n",
            "\n",
            "Summing up\n",
            "\n",
            "A CNN with kₓ = 1 and K(1, 1) = 1 can match the performance of a fully-connected network. The representation power of the filtered-activated image is least for kₓ = nₓ and K(a, b) = 1 for all a, b. Therefore, by tuning hyperparameter kₓ we can control the amount of information retained in the filtered-activated image. Also, by tuning K to have values different from 1 we can focus on different sections of the image. By doing both — tuning hyperparameter kₓ and learning parameter K, a CNN is guaranteed to have better bias-variance characteristics with lower bound performance equal to the performance of a fully-connected network. This can be improved further by having multiple channels.\n",
            "\n",
            "Extending the above discussion, it can be argued that a CNN will outperform a fully-connected network if they have same number of hidden layers with same/similar structure (number of neurons in each layer).\n",
            "\n",
            "However, this comparison is like comparing apples with oranges. An appropriate comparison would be to compare a fully-connected neural network with a CNN with a single convolution + fully-connected layer. Comparing a fully-connected neural network with 1 hidden layer with a CNN with a single convolution + fully-connected layer is fairer.\n",
            "\n",
            "MNIST data set in practice: a logistic regression model learns templates for each digit. This achieves good accuracy, but it is not good because the template may not generalize very well. A CNN with a fully connected network learns an appropriate kernel and the filtered image is less template-based. A fully-connected network with 1 hidden layer shows lesser signs of being template-based than a CNN.\n",
            "\n",
            "References\n",
            "\n",
            "Sigmoid: https://www.researchgate.net/figure/Logistic-curve-From-formula-2-and-figure-1-we-can-see-that-regardless-of-regression_fig1_301570543\n",
            "\n",
            "Tanh: http://mathworld.wolfram.com/HyperbolicTangent.html\n",
            "\n",
            "---\n",
            "\n",
            "Neural networks generate a lot of interest. However, it’s not always clear to people outside of the machine learning community the problems they’re suited for, what they are, or how they’re built. We’ll address these topics in this blog post, aiming to make neural networks accessible to all readers. For those with programming experience, I’ve appended a Jupyter Notebook at the end which you can follow to build your own neural network.\n",
            "\n",
            "Most commercially successful applications of neural networks are in the area of supervised learning. In supervised learning, we are trying to build a model that maps inputs to outputs. Some examples include:\n",
            "\n",
            "We can represent these models as function that takes an input and produces an output:\n",
            "\n",
            "y = F(x)\n",
            "\n",
            "where x is the input, y is the output, and F() is our model. Neural networks are a particularly effective way to build a model (i.e. F() ) for many classes of problems.\n",
            "\n",
            "Let’s briefly consider a traditional approach for building many models. We can derive models describing many phenomena by applying our understanding of calculus to specific domain knowledge. In physics, this would include Newton’s Laws of Motion, or conservation laws stating that mass, energy, and momentum are conserved in a closed system. This approach let’s successfully build a variety of important models, such as the ideal rocket equations which tell us how much fuel a rocket needs to reach space, or the Boussinesq equations which let us model waves along the coast.\n",
            "\n",
            "What about problems for which we don’t have intuition into the fundamental dynamics? Say you are building an autonomous vehicle, and want to recognize other cars on the road using a video stream from your dashboard camera. Despite the fact that we’re all quite good at recognizing cars, we haven’t been able to formulate physical principles describing what a car looks like. We can’t point to a combination of wheels, doors, and windows which make up a car. Neural networks provide us a technique which we can use to solve these types of problems effectively.\n",
            "\n",
            "Neural networks work by learning the mapping from input to output directly from data. The process of having the neural network learn this mapping is known as training. Training requires a dataset of training examples, which are pairs of inputs and the corresponding outputs (x and y). For training to be effective, we need a large dataset, typically tens of thousands to tens of millions of training examples.\n",
            "\n",
            "During training, we are optimizing the weights (or parameters) of the neural network. For each training example, we run the model on the input, and compare the model output to the target output using a loss function. Using an algorithm called backpropogation (or backprop for short), we update all the weights in the network so that the model output will be closer to the target output. The weights are updated proportionally to how much they contribute to any mismatch. We continue cycling through our training set, iteratively updating the model, until performance no longer increases.\n",
            "\n",
            "Let’s look at a visualization of a straightforward neural network. On the left hand side we have the input layer. This is our data, such as the pixels of an image, or how many times certain words appear in an email. Next we have two hidden layers. Hidden layers is a term we refer to the layers between the input and the output layers. Finally, we have the output layer. As the input passes through each layer of the neural network, it undergoes a series of computations. Each ‘unit’ (or ‘neuron’) in the hidden layers and the output layer contain a set of weights to be optimized, which control these calculations.\n",
            "\n",
            "Designing a neural network requires selecting hyper-parameters controlling both the architecture of the network and the training process. We use the term hyperparameters, since the term parameters is an alternative to weights. Hyperparameters related to the architecture include the number of layers, the width of each layer (i.e. number of units), as well the choice of something called the activation function in the units. The training is particularly influenced by the choice of optimization algorithm, the learning rate used by the algorithm, and whether a technique called regularization is implemented.\n",
            "\n",
            "Unfortunately, there is no way to know beforehand what is the best architecture for your problem, or what the best parameters for training that architecture would be. Practitioners are guided by a combination of experience, intuition, and best practices from the community. As the field is moving rapidly, this requires continuously staying up to date. Often the best way to start on a problem is to look at the machine learning literature to see if someone has solved a problem similar to yours, and take their solution as a starting point. Getting a solution to your particular problem will usually require several iterations of looking at the data, realizing ideas, modifying the model code, and testing.\n",
            "\n",
            "A key question of any neural network is how well is it able to perform on data it hasn’t seen before. Therefore, before training a neural network, a small portion of the data is set aside in what is commonly referred to as the test set. Following training a neural network, we compare the performance of the neural network on the training set and the test set. One possible scenario is that the model doesn’t perform well even on the data it’s trained on. In case, we say the model has high bias. When the model doesn’t fit the data it has seen well, the hyperparameters should be reevaluated. Another outcome is that the model performs well on the training set, but not very well on the test set. In this situation, we say the model has high variance — that the neural network has been overfit to the training data. Equivalently, the network hasn’t learnt features of the data that generalize well, and perhaps has memorized features specific to the training set. To address high variance, we typically employ a technique called regularization. When feasible, acquiring additional data is also beneficial.\n",
            "\n",
            "An important detail is that the data you train your neural network has to be similar to the data you apply your neural network to. Statistically, you want your training and test data to come from the same distribution. Intuitively, this means if you train your autonomous vehicle to drive exclusively in sunny weather, you can’t expect it to stay on the road during a snowfall. In practice, this means if you develop a neural network to predict customer behaviour, you’ll need to update your model periodically as your important factors such as your products, customers, and competition evolve.\n",
            "\n",
            "These are the broad concepts key to understanding how to apply neural networks, and communicate with those who regularly work with them. After reading this, you should be able to confidently navigate a conversation on applying neural networks.\n",
            "\n",
            "To summarize the key points about neural networks:\n",
            "\n",
            "They are an effective way to build a model mapping from input to output directly from a training dataset. Neural networks have many hyperparameters, and designing a good network is an iterative process. Your training and test data should come from the same distribution.\n",
            "\n",
            "Finally, a lot of software developers I speak to are keen to know implementation details of neural networks. The following Jupyter Notebook detail has been designed with you in mind, implementing a neural network to recognize handwritten digits with 98% accuracy. We’ll assume you have python environment setup with PyTorch. If you don’t, Anaconda is the recommended package manager and pretty simple to get started with.\n",
            "\n",
            "You can download the jupyter notebook to run on your own machine here, as well as follow along with the precomputed version here.\n"
          ]
        }
      ],
      "source": [
        "print(Values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XvMNErvH4Bo",
        "outputId": "d7d70e63-845d-4548-b667-68452b4d7341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinforcement Learning Introduction\n",
            "Reinforcement Learning : Markov-Decision Process (Part 1)\n",
            "Reinforcement Learning is full of Manipulative Consultants\n",
            "Reinforcement Learning — Model Based Planning Methods\n"
          ]
        }
      ],
      "source": [
        "print(Keys[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wOhbEJJIH6Xs",
        "outputId": "f426a656-8f87-4bcf-88ba-26bdd29233cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinforcement Learning Introduction\n",
            "\n",
            "An introduction to reinforcement learning problems and solutions Y Tech · Follow 4 min read · Jul 25, 2019 -- Share\n",
            "\n",
            "This post will be an introductory level on reinforcement learning. Throughout this post, the problem definitions and some most popular solutions will be discussed. After this article, you should be able to understand what is reinforcement learning, and how to find the optimal policy for the problem.\n",
            "\n",
            "The Problem Description\n",
            "\n",
            "The agent-environment interaction in reinforcement learning\n",
            "\n",
            "The Setting\n",
            "\n",
            "The reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.\n",
            "\n",
            "learning to interact with its environment. At each time step, the agent receives the environment’s state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state .\n",
            "\n",
            "(the environment presents a situation to the agent), and the agent must choose an appropriate in response. One time step later, the agent receives a (the environment indicates whether the agent has responded appropriately to the state) and a new . All agents have the goal to maximize the expected cumulative reward.\n",
            "\n",
            "Episodic vs. Continuing Tasks\n",
            "\n",
            "Continuing tasks are tasks that continue forever, without end.\n",
            "\n",
            "are tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point.\n",
            "\n",
            "* In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.\n",
            "\n",
            "* Episodic tasks come to an end whenever the agent reaches a terminal state.\n",
            "\n",
            "Cumulative Reward\n",
            "\n",
            "The discounted return at time step t is G(t) = R(t+1) + γ*R(t+2) + γ^2*R(t+3) + ...\n",
            "\n",
            "The agent selects actions with the goal of maximizing expected (discounted) return.\n",
            "\n",
            "The discount rate γ is something that you set, to refine the goal that you have the agent.\n",
            "\n",
            "* It must satisfy 0 ≤ γ ≤ 1 .\n",
            "\n",
            "* If γ = 0 , the agent only cares about the most immediate reward.\n",
            "\n",
            "* If γ = 1 , the return is not discounted.\n",
            "\n",
            "* For larger values of γ , the agent cares more about the distant future. Smaller values of γ result in more extreme discounting.\n",
            "\n",
            "MDPs and One-Step Dynamics\n",
            "\n",
            "---\n",
            "\n",
            "#InsideRL\n",
            "\n",
            "In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question for this blog is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.\n",
            "\n",
            "Typical Reinforcement Learning cycle\n",
            "\n",
            "Before we answer our root question i.e. How we formulate RL problems mathematically (using MDP), we need to develop our intuition about :\n",
            "\n",
            "The Agent-Environment relationship\n",
            "\n",
            "Markov Property\n",
            "\n",
            "Markov Process and Markov chains\n",
            "\n",
            "Markov Reward Process (MRP)\n",
            "\n",
            "Bellman Equation\n",
            "\n",
            "Markov Reward Process\n",
            "\n",
            "Grab your coffee and don’t stop until you are proud!🧐\n",
            "\n",
            "The Agent-Environment Relationship\n",
            "\n",
            "First let’s look at some formal definitions :\n",
            "\n",
            "Agent : Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions. Environment :It is the demonstration of the problem to be solved.Now, we can have a real-world environment or a simulated environment with which our agent will interact.\n",
            "\n",
            "Demonstrating an environment with which agents are interacting.\n",
            "\n",
            "State : This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.\n",
            "\n",
            "Anything that the agent cannot change arbitrarily is considered to be part of the environment. In simple terms, actions can be any decision we want the agent to learn and state can be anything which can be useful in choosing actions. We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it’s reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent. Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik’s cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it’s knowledge.\n",
            "\n",
            "The Markov Property\n",
            "\n",
            "Transition : Moving from one state to another is called Transition. Transition Probability: The probability that the agent will move from one state to another is called transition probability.\n",
            "\n",
            "The Markov Property state that :\n",
            "\n",
            "“Future is Independent of the past given the present”\n",
            "\n",
            "Mathematically we can express this statement as :\n",
            "\n",
            "Markov Property\n",
            "\n",
            "S[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that the transition from state S[t] to S[t+1] is entirely independent of the past. So, the RHS of the Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.\n",
            "\n",
            "State Transition Probability :\n",
            "\n",
            "As we now know about transition probability we can define state Transition Probability as follows :\n",
            "\n",
            "For Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by\n",
            "\n",
            "State Transition Probability\n",
            "\n",
            "We can formulate the State Transition probability into a State Transition probability matrix by :\n",
            "\n",
            "State Transition Probability Matrix\n",
            "\n",
            "Each row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each row is equal to 1.\n",
            "\n",
            "Markov Process or Markov Chains\n",
            "\n",
            "Markov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],….S[n] with a Markov Property.So, it’s basically a sequence of states with the Markov Property.It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).\n",
            "\n",
            "But what random process means ?\n",
            "\n",
            "To answer this question let’s look at a example:\n",
            "\n",
            "Markov chain\n",
            "\n",
            "The edges of the tree denote transition probability. From this chain let’s take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.\n",
            "\n",
            "Some samples from the chain :\n",
            "\n",
            "Sleep — Run — Ice-cream — Sleep\n",
            "\n",
            "Sleep — Ice-cream — Ice-cream — Run\n",
            "\n",
            "In the above two sequences what we see is we get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain.Hope, it’s now clear why Markov process is called random set of sequences.\n",
            "\n",
            "Before going to Markov Reward process let’s look at some important concepts that will help us in understand MRPs.\n",
            "\n",
            "Reward and Returns\n",
            "\n",
            "Rewards are the numerical values that the agent receives on performing some action at some state(s) in the environment. The numerical value can be positive or negative based on the actions of the agent. In Reinforcement learning, we care about maximizing the cumulative reward (all the rewards agent receives from the environment) instead of, the reward agent receives from the current state(also called immediate reward). This total sum of reward the agent receives from the environment is called returns.\n",
            "\n",
            "We can define Returns as :\n",
            "\n",
            "Returns (Total rewards from the environment)\n",
            "\n",
            "r[t+1] is the reward received by the agent at time step t[0] while performing an action(a) to move from one state to another. Similarly, r[t+2] is the reward received by the agent at time step t[1] by performing an action to move to another state. And, r[T] is the reward received by the agent by at the final time step by performing an action to move to another state.\n",
            "\n",
            "Episodic and Continuous Tasks\n",
            "\n",
            "Episodic Tasks: These are the tasks that have a terminal state (end state).We can say they have finite states. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends!). This is called an episode. Once we restart the game it will start from an initial state and hence, every episode is independent. Continuous Tasks : These are the tasks that have no ends i.e. they don’t have any terminal state.These types of tasks will never end.For example, Learning how to code!\n",
            "\n",
            "Now, it’s easy to calculate the returns from the episodic tasks as they will eventually end but what about continuous tasks, as it will go on and on forever. The returns from sum up to infinity! So, how we define returns for continuous tasks?\n",
            "\n",
            "This is where we need Discount factor(ɤ).\n",
            "\n",
            "Discount Factor (ɤ): It determines how much importance is to be given to the immediate reward and future rewards. This basically helps us to avoid infinity as a reward in continuous tasks. It has a value between 0 and 1. A value of 0 means that more importance is given to the immediate reward and a value of 1 means that more importance is given to future rewards. In practice, a discount factor of 0 will never learn as it only considers immediate reward and a discount factor of 1 will go on for future rewards which may lead to infinity. Therefore, the optimal value for the discount factor lies between 0.2 to 0.8.\n",
            "\n",
            "So, we can define returns using discount factor as follows :(Let’s say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)\n",
            "\n",
            "Returns using discount factor\n",
            "\n",
            "Let’s understand it with an example,suppose you live at a place where you face water scarcity so if someone comes to you and say that he will give you 100 liters of water!(assume please!) for the next 15 hours as a function of some parameter (ɤ).Let’s look at two possibilities : (Let’s say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)\n",
            "\n",
            "One with discount factor (ɤ) 0.8 :\n",
            "\n",
            "Discount Factor (0.8)\n",
            "\n",
            "This means that we should wait till 15th hour because the decrease is not very significant , so it’s still worth to go till the end.This means that we are also interested in future rewards.So, if the discount factor is close to 1 then we will make a effort to go to end as the reward are of significant importance.\n",
            "\n",
            "Second, with discount factor (ɤ) 0.2 :\n",
            "\n",
            "Discount Factor (0.2)\n",
            "\n",
            "This means that we are more interested in early rewards as the rewards are getting significantly low at hour.So, we might not want to wait till the end (till 15th hour) as it will be worthless.So, if the discount factor is close to zero then immediate rewards are more important that the future.\n",
            "\n",
            "So which value of discount factor to use ?\n",
            "\n",
            "It depends on the task that we want to train an agent for. Suppose, in a chess game, the goal is to defeat the opponent’s king. If we give importance to the immediate rewards like a reward on pawn defeat any opponent player then the agent will learn to perform these sub-goals no matter if his players are also defeated. So, in this task future rewards are more important. In some, we might prefer to use immediate rewards like the water example we saw earlier.\n",
            "\n",
            "Markov Reward Process\n",
            "\n",
            "Till now we have seen how Markov chain defined the dynamics of a environment using set of states(S) and Transition Probability Matrix(P).But, we know that Reinforcement Learning is all about goal to maximize the reward.So, let’s add reward to our Markov Chain.This gives us Markov Reward Process.\n",
            "\n",
            "Markov Reward Process : As the name suggests, MDPs are the Markov chains with values judgement.Basically, we get a value from every state our agent is in.\n",
            "\n",
            "Mathematically, we define Markov Reward Process as :\n",
            "\n",
            "Markov Reward Process\n",
            "\n",
            "What this equation means is how much reward (Rs) we get from a particular state S[t]. This tells us the immediate reward from that particular state our agent is in. As we will see in the next story how we maximize these rewards from each state our agent is in. In simple terms, maximizing the cumulative reward we get from each state.\n",
            "\n",
            "We define MRP as (S,P, R,ɤ) , where :\n",
            "\n",
            "S is a set of states,\n",
            "\n",
            "P is the Transition Probability Matrix,\n",
            "\n",
            "R is the Reward function, we saw earlier,\n",
            "\n",
            "ɤ is the discount factor\n",
            "\n",
            "Markov Decision Process\n",
            "\n",
            "Now, let’s develop our intuition for Bellman Equation and Markov Decision Process.\n",
            "\n",
            "Policy Function and Value Function\n",
            "\n",
            "Value Function determines how good it is for the agent to be in a particular state. Of course, to determine how good it will be to be in a particular state it must depend on some actions that it will take. This is where policy comes in. A policy defines what actions to perform in a particular state s.\n",
            "\n",
            "A policy is a simple function, that defines a probability distribution over Actions (a∈ A) for each state (s ∈ S). If an agent at time t follows a policy π then π(a|s) is the probability that the agent with taking action (a ) at a particular time step (t).In Reinforcement Learning the experience of the agent determines the change in policy. Mathematically, a policy is defined as follows :\n",
            "\n",
            "Policy Function\n",
            "\n",
            "Now, how do we find a value of a state. The value of state s, when the agent is following a policy π which is denoted by vπ(s) is the expected return starting from s and following a policy π for the next states until we reach the terminal state. We can formulate this as :(This function is also called State-value Function)\n",
            "\n",
            "Value Function\n",
            "\n",
            "This equation gives us the expected returns starting from the state(s) and going to successor states thereafter, with the policy π. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic. It is the expectation of returns from start state s and thereafter, to any other state. And also note that the value of the terminal state (if there is any) is zero. Let’s look at an example :\n",
            "\n",
            "Example\n",
            "\n",
            "Suppose our start state is Class 2, and we move to Class 3 then Pass then Sleep.In short, Class 2 > Class 3 > Pass > Sleep.\n",
            "\n",
            "Our expected return is with a discount factor of 0.5:\n",
            "\n",
            "Calculating the Value of Class 2\n",
            "\n",
            "Note:It’s -2 + (-2 * 0.5) + 10 * 0.25 + 0 instead of -2 * -2 * 0.5 + 10 * 0.25 + 0.Then the value of Class 2 is -0.5 .\n",
            "\n",
            "Bellman Equation for Value Function\n",
            "\n",
            "Bellman Equation helps us to find optimal policies and value functions. We know that our policy changes with experience so we will have different value functions according to different policies. The optimal value function is one that gives maximum value compared to all other value functions.\n",
            "\n",
            "Bellman Equation states that value function can be decomposed into two parts:\n",
            "\n",
            "Immediate Reward, R[t+1]\n",
            "\n",
            "Discounted value of successor states,\n",
            "\n",
            "Mathematically, we can define Bellman Equation as :\n",
            "\n",
            "Bellman Equation for Value Function\n",
            "\n",
            "Let’s understand what this equation says with a help of an example :\n",
            "\n",
            "Suppose, there is a robot in some state (s) and then he moves from this state to some other state (s’). Now, the question is how good it was for the robot to be in the state(s). Using the Bellman equation, we can that it is the expectation of reward it got on leaving the state(s) plus the value of the state (s’) he moved to.\n",
            "\n",
            "Let’s look at another example :\n",
            "\n",
            "Backup Diagram\n",
            "\n",
            "We want to know the value of state s. The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.\n",
            "\n",
            "Value Calculation\n",
            "\n",
            "The above equation can be expressed in matrix form as follows :\n",
            "\n",
            "Bellman Linear Equation\n",
            "\n",
            "Where v is the value of state we were in, which is equal to the immediate reward plus the discounted value of the next state multiplied by the probability of moving into that state.\n",
            "\n",
            "The running time complexity for this computation is O(n³). Therefore, this is clearly not a practical solution for solving larger MRPs (same for MDPs).In later Blogs, we will look at more efficient methods like Dynamic Programming (Value iteration and Policy iteration), Monte-Claro methods, and TD-Learning.\n",
            "\n",
            "We are going to talk about the Bellman Equation in much more detail in the next story.\n",
            "\n",
            "What is Markov Decision Process ? Markov Decision Process : It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.\n",
            "\n",
            "It is a tuple of (S, A, P, R, 𝛾) where:\n",
            "\n",
            "S is a set of states,\n",
            "\n",
            "A is the set of actions agent can choose to take,\n",
            "\n",
            "P is the transition Probability Matrix,\n",
            "\n",
            "R is the Reward accumulated by the actions of the agent,\n",
            "\n",
            "𝛾 is the discount factor.\n",
            "\n",
            "P and R will have slight change w.r.t actions as follows :\n",
            "\n",
            "Transition Probability Matrix\n",
            "\n",
            "Transition Probability Matrix w.r.t action\n",
            "\n",
            "Reward Function\n",
            "\n",
            "Reward Function w.r.t action\n",
            "\n",
            "Now, our reward function is dependent on the action.\n",
            "\n",
            "Till now we have talked about getting a reward (r) when our agent goes through a set of states (s) following a policy π. Actually, in Markov Decision Process(MDP) the policy is the mechanism to take decisions. So now we have a mechanism that will choose to take an action.\n",
            "\n",
            "Policies in an MDP depend on the current state. They do not depend on history. That’s the Markov Property. So, the current state we are in characterizes history.\n",
            "\n",
            "We have already seen how good it is for the agent to be in a particular state(State-value function). Now, let’s see how good it is to take a particular action following a policy π from state s (Action-Value Function).\n",
            "\n",
            "State-action value function or Q-Function\n",
            "\n",
            "This function specifies how good it is for the agent to take action (a) in a state (s) with a policy π.\n",
            "\n",
            "Mathematically, we can define the State-action value function as :\n",
            "\n",
            "State-action value function\n",
            "\n",
            "Basically, it tells us the value of performing a certain action(a) in a state(s) with a policy π.\n",
            "\n",
            "Let’s look at an example of the Markov Decision Process :\n",
            "\n",
            "Example of MDP\n",
            "\n",
            "Now, we can see that there are no more probabilities. In fact, now our agent has choices to make like after waking up, we can choose to watch Netflix or code and debug. Of course, the actions of the agent are defined w.r.t some policy π and will get the reward accordingly.\n",
            "\n",
            "---\n",
            "\n",
            "Reinforcement Learning is full of Manipulative Consultants\n",
            "\n",
            "Manipulative Consultant\n",
            "\n",
            "Imagine you go to an investment consultant, and you first ask how he charges.\n",
            "\n",
            "Is it according to the profit you’ll make?\n",
            "\n",
            "“No,” he says. “The more accurate I am in my predictions of your returns, you’ll pay me more. But I will be tested only on the investments you choose to make.”\n",
            "\n",
            "This smells a bit fishy, and you start sniffing around for other people who are using this consultant.\n",
            "\n",
            "Turns out he recommended them all only government bonds with low return and low variability.\n",
            "\n",
            "He even told them this has the highest mean return!\n",
            "\n",
            "They all believed him, bought the bonds, and of course he was pretty accurate about the return, with very small errors. So they had to pay him his maximum fee.\n",
            "\n",
            "Source: South Park\n",
            "\n",
            "What do you think about this guy? I think he is a kind of “Manipulative Consultant”.\n",
            "\n",
            "\n",
            "\n",
            "And everyone in Reinforcement Learning is using just this guy.\n",
            "\n",
            "Currently, in Reinforcement Learning (RL) there are two leading families of algorithms: Deep Q Networks (DQN) and Actor Critic. Both are using a consultant function or a ‘value estimation’ functions — a Deep Neural Network (DNN) which estimates the value of a state and/or action. In DQN it’s the Q-network, and in Actor Critic it’s the Critic network. This is basically a good decision: value-estimation functions can learn off-policy, meaning they can learn from watching someone else play, even if he’s not so good. This enables them to learn from the experience collected using past policies which have already been abandoned.\n",
            "\n",
            "However, there’s a catch: we “pay” this consultant according to his accuracy: the loss function which is used to optimize the network is based on the network’s prediction error. And the network is tested on the actions it chose: the policy will do what the network advised as best, and this will be the only future source of experience.\n",
            "\n",
            "Now, everybody complains that RL doesn’t work yet and that Deep is hardly helping. And rightly so. Training a RL algorithm is brittle: it depends strongly on the initialization of the network and of the parameters, so you have to repeat the same experiment again and again, each time initialized differently. You see your algorithm improving and then regressing. You’re puzzled, because it does so while the loss function continues showing improved performance. You can choose the best temporary network along the way and call it a day, but there is nothing you can do through RL to further improve the results.\n",
            "\n",
            "So what we claim here is, that you simply chose the wrong consultant. Or at least — chose the wrong way to pay him. He’s choosing low-rewards actions, and tells you all other options are worse. He will be more accurate because the rewards on the actions he recommends are so predictable. And you’ll never catch him manipulating you, because you keep testing him on what he chose.\n",
            "\n",
            "First, let’s demonstrate that these loss-gaps do indeed exist. Take a simple game: two slots machines (or “Multi-Armed Bandit” as they’re called in RL), the right one gives 1 reward but with high variance, and the left one is broken, so it gives 0 reward with 0 variance. We call it the Broken-Armed-Bandit.\n",
            "\n",
            "Now, you have to decide which one to use in each episode of the game. Seems easy? Not for Q-learning.\n",
            "\n",
            "Take a look on the two thin lines in the graph below. They show the update terms of the Q-table of the agents that are currently choosing the right handle (thin line, green) and of those currently choosing the left handle (thin line, red). In DQN, this update term will be the function loss. It is clear from the graph that those choosing left are doing much better and will incur lower loss:\n",
            "\n",
            "Now, every good RL algorithm has its exploration scheme. Here we used the epsilon-greedy scheme, with a decaying epsilon. And indeed, with 100% exploration it tests the consultant on things the consultant didn’t recommend, and it’s getting basically the same loss. But this is true only at the beginning of the training. As the epsilon decays, the exploration decreases, and the red thin line keeps attenuating. Now if you saw that line in a real training, wouldn’t you think everything is great since the loss is declining? Actually, what you’re watching is a lazy network being freed of the hard tests of the exploration.\n",
            "\n",
            "What we saw is a gap in the loss, where the boring decisions are winning. When we optimize a deep-network by minimizing this loss, sometimes it will favor the boring decisions to minimize its loss. But what if we don’t use DNN at all? What if we use good old Q-learning, with a Q- table?\n",
            "\n",
            "There is still a problem, and it is called the “Boring Areas Trap”.\n",
            "\n",
            "Boring Areas Trap\n",
            "\n",
            "Imagine you have a bicycle, and someone is giving you a free pizza a mile away from your home. You have two options: you can give up on riding there, and you get a mean of 0 pizza with 0 variance. On the other hand you can decide to ride there, and then you get a mean of 1 pizza, but with high variance: with a very small probability, you may have an accident and you’ll spend six months in a cast, in agonizing pains, losing money for your ruined bicycle, and with no pizza.\n",
            "\n",
            "Normally, this is a simple decision: you never had a car accident before, you estimate the chances of it happening now as low, and you prefer the higher pizza mean. So you go there and get the pizza.\n",
            "\n",
            "But what if you’re unlucky, and after only 100 rides you had an accident? Now you estimate the chances that an accident can happen to be much higher than the true probability. The estimated mean reward from driving to the free pizza becomes negative, and you decide to stay home.\n",
            "\n",
            "Now here is the catch: you will never ride again, and hence will never change your mind about riding. You will keep believing it has negative mean reward, you’re experience from staying home will validate your beliefs about the mean return of staying home, and nothing will change.\n",
            "\n",
            "Why should you get out of home anyway? Well, what has to happen is a complementary error. For example, you stay at home and a shelf falls on your head. Once again, agonizing pain. Now, you have no one to blame but your shelf. Your estimation of staying at home is becoming negative too. And if it is lower than your estimation of leaving home, you will go out again for that pizza.\n",
            "\n",
            "Note that there was no optimization involved: You had a Q-table of one state: a hungry state, and two actions: go or no-go to the pizza. You calculated the means directly from the rewards you got. This was the best thing you could do, but you ended up stuck at home, hungry, until this shelf got you out.\n",
            "\n",
            "This phenomenon can be simulated with the same Broken-Armed-Bandit from above. But now we can try and solve it using Q-learning.\n",
            "\n",
            "Let’s look at 10 agents training on this task:\n",
            "\n",
            "We can see that all of them, at some point, go to gain zero reward, meaning they choose to pull the malfunctioning arm. Imagine them, standing in a line, pulling the dead machine arm, ignoring the working machine with all the lights to its right. Don’t they look stupid? Well, the joke is on us for using them as our experts. Note: to speed up things, we chose a high learning rate of 0.1, so things that usually happen after millions of iterations will happen very quickly.\n",
            "\n",
            "\n",
            "\n",
            "Now, let’s take a hundred agents and look how many choose the left, nonworking arm. They are on the red line:\n",
            "\n",
            "Once again, it takes some time but all of them eventually choose the left arm as their best option.\n",
            "\n",
            "To see what’s going on, we will look at the inner parameters of one agent — the values of Q_left and Q_right in its Q-table. We removed all exploration to see what’s really happening, and initialized the parameters to be optimal, so this is a well-trained agent, at least at the start. The right arm has high variance as before. Here we gave a small variance to the left arm as well, so this is a regular two-armed-bandit problem with variance differences:\n",
            "\n",
            "The right arm has high variance. So its estimation Q_right has also high variance, though much lower since it is summed with past rewards. Q_right, because of a few concentrated bad rewards, becomes lower than Q_left at episode 40.\n",
            "\n",
            "From that point on, the agent chooses only the left handle. So it has entered the “Boring Areas Trap”. Now, Q_right can’t change, due to lack of examples. Q_left is hardly changing due to its low variance. And this, ladies and gentlemen, is why we call it a trap!\n",
            "\n",
            "At episode 320, the complementary error occurs. Q_left becomes lower than the falsely-low Q_right. This is when we get out of the trap and start pulling the right arm, getting better estimations of Q_right.\n",
            "\n",
            "What variance differences cause this problem? Here we can see a grades-map, for different values of σ_l and σ_r, showing how many agents out of 50 chose the right arm after 10,000 episodes:\n",
            "\n",
            "At the bottom-right there is a dark region where all agents fail, due to large variance differences. There is another area at the center where agents are flitting in and out of the trap, due to lower variance differences. Only when the variance differences are low, Q-learning is working. A lower learning rate will move the dark areas further to the right, but will, well, lower the learning rate, so training will be very slow.\n",
            "\n",
            "Reward noising\n",
            "\n",
            "The proposed solution comes from an experiment in human cognition. Some scientists conducted an experiment called “Agriculture on march” which is the same as the two-armed-bandit, but where each action moves both machines’ means. They found that adding a little noise to the reward paradoxically helps people ”rule out simple hypotheses” and encourages “sampling of alternatives”, and actually helps them gain more rewards!\n",
            "\n",
            "We can do the same here. We can add a symmetric noise to the reward, so it will not influence the mean reward.\n",
            "\n",
            "But if we add noise to all rewards equally, there will still be a loss gap in favor of the left machine. So we want it to be adaptive, meaning we’ll add noise only to the low-variance actions.\n",
            "\n",
            "If we do this we get the thick lines in the graph we have already seen:\n",
            "\n",
            "This shows that we added a lot of noise to all rewards, but now there is about the same amount of noise in both machines.\n",
            "\n",
            "This is what ASRN, or Adaptive Symmetric Reward Noising, does: it estimates which states/actions have low variance, and adds noise mainly to them. How does it estimate? Using the update to the Q_table. The bigger the update is, the more surprising the reward is, and the less noise it will get.\n",
            "\n",
            "You can see how it’s implemented here. Of course, ASRN has its own training period, so the changes only start after 1000 episodes in the above example.\n",
            "\n",
            "When we check the ASRN on the Broken-Armed-Bandit above, we see that it helps the agents get out of the boring-areas-trap. Here are the 10 agents from above:\n",
            "\n",
            "Some of them reached the Boring Areas Trap, but managed to escape using the noise we added.\n",
            "\n",
            "Driving with noise\n",
            "\n",
            "Now, all this is nice to use on bandits, but what about using it on some real stuff?\n",
            "\n",
            "Well, driving is a very suitable example. Just as with the pizza above, there is a strategy which will give you low reward with low variance, like “go left till you crash”. On the other hand, there is the strategy of actually driving, which can have a high mean reward due to the “reach the target” big prize, but it comes with high variance — there are many dangers waiting along the road. We trained an agent using the AirSim Neighborhood driving simulation. It is a great realistic driving simulator:\n",
            "\n",
            "and they already implemented a DQN agent. So all is left to do is to look at the mean driving time after plugging in the ASRN (green) compared to without ASRN (red) and with uniform reward noising (cyan):\n",
            "\n",
            "This is definitely better, isn’t it? You can see the changed code here .\n",
            "\n",
            "Here is a test drive with the best policy. It is not a very good driver. However it is quite an achievement for training only for 2750 games.\n",
            "\n",
            "To sum it all up: we saw the problems that variance differences cause to RL. Some are global, like the Boring Areas Trap, and some are specific to Deep Reinforcement Learning (DRL), like the Manipulative Consultant. We also saw that reward-noising can help a little, especially if the noising is symmetric and adaptive to the actual action variance. We explored Q-learning and DQN, but it is likely that it holds for Actor Critic and other algorithms too.\n",
            "\n",
            "Obviously, reward-noising is not a complete solution. A lot of sophisticated exploration needs to be done in parallel, together with other RL tricks like clipping and such. The Manipulative Consultant and Boring Areas Trap problems raise at least as many questions as they answer. But it is important to bear in mind those problems when we sit down to plan our RL strategy. It’s crucial to think: are there any variance differences in this environment? How are they affecting the chosen algorithm? And maybe this will lead to a more stable RL.\n",
            "\n",
            "Thanks to: Shlomo cohen, Talya Sohlberg, Orna Cohen, Gili Berk, and Gil Sod-Moriya\n",
            "\n",
            "· Machine Learning\n",
            "\n",
            "· Reinforcement Learning\n",
            "\n",
            "---\n",
            "\n",
            "Reinforcement Learning — Model Based Planning Methods\n",
            "\n",
            "In previous articles, we have talked about reinforcement learning methods that are all based on model-free methods, which is also one of the key advantages of RL learning, as in most cases learning a model of environment can be tricky and tough. But what if we want to learn a model of environment or what if we already have a model of environment and how can we leverage that to help the learning process? In this article, we will together explore RL methods with environment as a model. The following will be structured as:\n",
            "\n",
            "Start with basic idea of how to model an environment Implement an example in Python using the theory we just learnt Further ideas to extend the theory to more general cases\n",
            "\n",
            "Model the Environment\n",
            "\n",
            "An agent starts from a state, by taking an available action in that state, the environment gives it feedback, and accordingly the agent lands into next state and receive reward if any. In this general settings, the environment gives an agent two signals, one is its next state in the setting, and the other is reward. So when we say to model an environment, we are modelling a function mapping (state, action) to (nextState, reward) . For example, consider a situation in a grid world setting, an agent bashes its head into the wall, and in response, the agent stays where it is and gets a reward of 0, then in the simplest format, the model function will be (state, action)-->(state, 0) , indicating that the agent with this specific state and action, the agent will stay at the same place and get reward 0.\n",
            "\n",
            "Algorithm\n",
            "\n",
            "Let’s now look into how a model of environment can help improve the process of Q-learning. We start by introducing the simplest form of an algorithm called Dyna-Q:\n",
            "\n",
            "The way Q-learning leveraging models to backup policy is simple and straight forward. Firstly, the a, b, c, d steps are exactly the same as general Q-learning steps(if you are not familiar with Q-learning, please check out my examples here). The only difference lies in step e and f , in step e , a model of the environment is recorded based on the assumption of deterministic environment(for non-deterministic and more complex environment, a more general model can be formulated based on the particular case). Step f can be simply summarised as applying the model being learnt and update the Q function n times, where n is a predefined parameter. The backup in step f is totally the same as it is in step d , and you may think it as repeating what the agent has experienced several times in order to reinforce the learning process.\n",
            "\n",
            "Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the “final common path” for both learning and planning.\n",
            "\n",
            "The general Dyna Architecture\n",
            "\n",
            "The graph shown above more directly displays the general structure of Dyna methods. Notice the 2 upward arrows in Policy/value functons , which in most cases are Q functions that we talked before, one of the arrow comes from direct RL update through real experience , which in this case equals the agent exploring around the environment, and the other comes from planning update through simulated experience , which, in this case, is repeating the model the agent learnt from real experience . So in each action taking, the learning process is strengthened by updating the Q function from both actual action taking and model simulation.\n",
            "\n",
            "Implement Dyna Maze\n",
            "\n",
            "I believe the best way to understand an algorithm is to implement an actual example. I will take the example from reinforcement learning an introduction, implement it in Python and compare it with general Q learning without planning steps(model simulation).\n",
            "\n",
            "Game Setting\n",
            "\n",
            "Dyna Maze Board\n",
            "\n",
            "Consider the simple maze shown inset in the Figure. In each of the 47 states there are four actions, up , down , right , and left , which take the agent deterministically to the corresponding neighbouring states, except when movement is blocked by an obstacle or the edge of the maze, in which case the agent remains where it is. Reward is zero on all transitions, except those into the goal state, on which it is +1 . After reaching the goal state (G) , the agent returns to the start state (S) to begin a new episode.\n",
            "\n",
            "The whole structure of this implementation is to have 2 classes, the first class represents the board, which is also the environment, that is able to\n",
            "\n",
            "Take in an action and output the agent next state(or position) Give reward accordingly\n",
            "\n",
            "And the second class represents the agent, which is able to\n",
            "\n",
            "Explore around the board Keep track of a model of the environment Update the Q functions along the way.\n",
            "\n",
            "Board Implementation\n",
            "\n",
            "The first class of the board settings are similar with many board games we talked before, you can checkout full implementation here. I will eliminate my explanation here(you can check my previous articles to see more examples), as a result, we will have a board look like this:\n",
            "\n",
            "Board Implementation\n",
            "\n",
            "The board is represented in an numpy array, where z indicates the block, * indicates the agent’s current position and 0 indicates empty and available spots.\n",
            "\n",
            "Agent Implementation\n",
            "\n",
            "Initilisation\n",
            "\n",
            "Firstly, in the init function we will initialise all the parameters required for the algorithm.\n",
            "\n",
            "Besides those general Q-learning settings(learning rate, state_actions, …), a model of (state, action) -> (reward, state) is also initialised as python dictionary, and the model will only be updated along with the agent’s exploration in the environment. The self.steps is the number of time the model is used to update the Q function in each action taking, and self.steps_per_episode is used to record the number of steps in each episode(we will take it as a key metrics in the following algorithm comparison).\n",
            "\n",
            "Choose Actions\n",
            "\n",
            "In the chooseAction function, the agent will still take ϵ-greedy action, where it has self.exp_rate probability to take a random action and 1 — self.exp_rate probability to take a greedy action.\n",
            "\n",
            "Model learning and Policy updating\n",
            "\n",
            "Now let’s get to the key point of policy updating using models being learnt along agent’s exploration.\n",
            "\n",
            "This implementation follows exactly as the algorithm we listed above. At each episode(game playing), after the first round of Q function update, the model will also be updated with self.model[self.state][action]=(reward, nxtState) , and then Q function will be repeatedly updated by self.steps number of times. Notice that in side the loop, the state and action are both randomly selected from the previously observations.\n",
            "\n",
            "Experimenting with different steps\n",
            "\n",
            "When the number of steps is set to 0, the Dyna-Q method is essentially Q-learning. Let’s compare the learning process with steps of 0, 5 and 50.\n",
            "\n",
            "Dyna-Q with different steps\n",
            "\n",
            "The x-axis is the number of episodes and y-axis is the number of steps to reach the goal. The task is to get to the goal as fast as possible. From the learning curve, we observe that the learning curve of planning agent(with simulated model) stabilises faster than non-planing agent. Referring to the words in Sutton’s book:\n",
            "\n",
            "Without planning (n = 0), each episode adds only one additional step to the policy, and so only one step (the last) has been learned so far. With planning, again only one step is learned during the first episode, but here during the second episode an extensive policy has been developed that by the end of the episode will reach almost back to the start state\n",
            "\n",
            "The additional model simulation and backup further reinforced the agent’s experience, thus resulted in a faster and more stable learning process.(checkout the full implementation)\n",
            "\n",
            "How to generalise the idea?\n",
            "\n",
            "The example we explored here surely has limited use, as the state is discretised an the action is deterministic. But the idea of modelling environment to accelerate learning process has unlimited use.\n",
            "\n",
            "For discrete states with non-deterministic action\n",
            "\n",
            "A probability model could be learnt rather than a straight forward 1 to 1 mapping we introduced above. The probability model should be constantly updated through the learning process, and during the backup stage, the (reward, nextState) could be chosen non-deterministically with a probability distribution.\n",
            "\n",
            "For continuous states\n",
            "\n",
            "The Q function update will be slightly different(I will introduce it in further articles), and the key would be to learn a more complex and general parametric model of the environment. This process could involve general supervised learning algorithms with current state, action as input and next state and reward as output.\n",
            "\n",
            "In next post, we will learn further ideas to improve Dyna methods and talk about situations when the model is wrong!\n",
            "\n",
            "And lastly, please check out the full code here. You are welcomed to contribute, and if you have any questions or suggestions, please raise comment below!\n",
            "\n",
            "Reference:\n"
          ]
        }
      ],
      "source": [
        "print(Values[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSUd7AKcH8xy",
        "outputId": "065494c3-1fdc-4fb8-c4c6-3211c8f68b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploy ML models at scale\n",
            "A Recipe for using Open Source Machine Learning models\n",
            "Build and Deploy a Deep Learning Image Classification App\n",
            "Deploy Machine Learning Web API using NGINX and Docker on Kubernetes in Python\n"
          ]
        }
      ],
      "source": [
        "print(Keys[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bo3GUdkwH-Oy",
        "outputId": "7164d0a2-7289-443c-ee4f-4affca7ef02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploy ML models at scale\n",
            "\n",
            "ML Model Deployment (Source)\n",
            "\n",
            "Let’s assume that you have built a ML model and that you are happy with its performance. Then the next step is to deploy the model into production. In this blog series I will cover how you can deploy your model for large scale consumption with in a scalable Infrastructure using AWS using docker container service.\n",
            "\n",
            "In this blog I will start with the first step of building an API framework for the ML model and running it in you local machine. For the purpose of this blog, let’s consider the Sentiment classification model built here. In order to deploy this model we will follow the below steps:\n",
            "\n",
            "Convert the model into .hdf5 file or .pkl file Implement a Flask API Run the API\n",
            "\n",
            "Convert the model into “.hdf5” file or ‘.pkl’ file\n",
            "\n",
            "In case the model is a built on sklearn, it would be best to save it as a ‘.pkl’ file. Alternatively if it is a deep learning model then it is recommended to save the model as a ‘HDF’ file. The main difference between ‘.pkl’ and ‘.hdf’ is, pickle requires a large amount of memory to save a data structure to disk, where as HDF is designed to efficiently store large data sets.\n",
            "\n",
            "Save model in Pickle(.pkl):\n",
            "\n",
            "from sklearn.externals import joblib # Save to file # Fit the model (example of a model in sklearn)\n",
            "\n",
            "model = LogisticRegression()\n",
            "\n",
            "model.fit(X_train, Y_train) #working directory\n",
            "\n",
            "joblib_file = \"best_model.pkl\"\n",
            "\n",
            "joblib.dump(model, joblib_file)\n",
            "\n",
            "Save model in HDF(.hdf5):\n",
            "\n",
            "Once you have trained your deep learning model in keras or tensorflow you can save the model architecture and its weights using a .hdf5 file system.\n",
            "\n",
            "# save the best model and early stopping saveBestModel = keras.callbacks.ModelCheckpoint('/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
            "\n",
            "Implement a Flask API\n",
            "\n",
            "Step 1: Load the saved model (as per previous section) by one of the following methods depending on the type of file i.e. ‘hdf5’ or ‘pkl’.\n",
            "\n",
            "HDF5:\n",
            "\n",
            "from keras.models import load_model\n",
            "\n",
            "import h5py\n",
            "\n",
            "prd_model = load_model('best_model.hdf5')\n",
            "\n",
            "PKL:\n",
            "\n",
            "from sklearn.externals import joblib\n",
            "\n",
            "loaded_model = joblib.load('best_model.pkl')\n",
            "\n",
            "Step 2: Import flask and create a flask application object as shown below:\n",
            "\n",
            "from flask import Flask, url_for, request app = Flask(__name__)\n",
            "\n",
            "Step 3: The next step is to build a test API function which returns the “API working” string. This can be used to ensure the health of the API when it is deployed in production. Here we use ‘@app.route’, which is a python decorator ( a decorator is a function that takes another function and extends the behaviour of the latter function without explicitly modifying it)\n",
            "\n",
            "Step 4: The next step is to build a “POST” request api for processing requests to our sentiment model. We are using the path name “/sentiment”. The function reads the json input and converts it into pandas dataframe. It extracts the relevant fields from the json and calls the “get_sentiment_DL” function for processing. “get_sentiment_DL” function contains the trained model which has been loaded via ‘hdf5’ file. It finally will return back the results of the model in the form of json result.\n",
            "\n",
            "\n",
            "\n",
            ".route(‘/sentiment’, methods=[‘POST’])\n",
            "\n",
            "def sentiment():\n",
            "\n",
            "if request.method == ‘POST’:\n",
            "\n",
            "text_data = pd.DataFrame(request.json)\n",
            "\n",
            "text_out = get_sentiment_DL(prd_model, text_data, word_idx)\n",
            "\n",
            "\n",
            "\n",
            "text_out = text_out[[‘ref’,’Sentiment_Score’]] # main API code @app .route(‘/sentiment’, methods=[‘POST’])def sentiment():if request.method == ‘POST’:text_data = pd.DataFrame(request.json)text_out = get_sentiment_DL(prd_model, text_data, word_idx)text_out = text_out[[‘ref’,’Sentiment_Score’]] #Convert df to dict and then to Json\n",
            "\n",
            "text_out_dict = text_out.to_dict(orient=’records’)\n",
            "\n",
            "text_out_json = json.dumps(text_out_dict, ensure_ascii=False) return text_out_json\n",
            "\n",
            "Step 5: The detailed model processing steps will be performed by “get_sentiment_DL” function. In the case of our deep learning sentiment model we are passing:\n",
            "\n",
            "best_model: the loaded model\n",
            "\n",
            "2. text_data: Input text for sentiment classification\n",
            "\n",
            "3. word_idx: Word index from the GloVe file (details of the model here).\n",
            "\n",
            "def get_sentiment_DL(best_model, text_data, word_idx): '''Modle Processing''' return sentiment_score\n",
            "\n",
            "Step 6: Add the below section to run the app. Here Host is set as “0.0.0.0” as we are hosting in our local server. However you can configure it to your network settings. Debug can be set to True at the time of building the API functionality. The Port is set to 5005, however this can be configured as per your requirement.\n",
            "\n",
            "if __name__ == “__main__”:\n",
            "\n",
            "app.run(host=”0.0.0.0\", debug=False, port=5005)\n",
            "\n",
            "Run the API\n",
            "\n",
            "To run the API, open a command line window and go to the directory where the code is stored. Run the python script by running the below command (“sentiment.py” is the name of the file with the above API implementation).\n",
            "\n",
            "python sentiment.py\n",
            "\n",
            "On running the above command you will be able to see the below result in your command line window:\n",
            "\n",
            "Once the API is running you can test the API by going to your browser and typing “0.0.0.0:5005/apitest”. You will get the below result in you browser.\n",
            "\n",
            "Browser — API Test\n",
            "\n",
            "You can now pass any data to the API using python as shown below. The address in our case is “http://0.0.0.0:5005/sentiment”. The results of the model will be returned and stored in “response” field.\n",
            "\n",
            "import requests\n",
            "\n",
            "import json\n",
            "\n",
            "import urllib.parse data_json = '''{\"ref\":[1], \"text\":[\"I am well\"]}''' head= {“Content-type”: “application/json”} response = requests.post(‘ http://0.0.0.0:5005/sentiment' , json = data_json, headers = head) result = response.content.decode(‘utf8’)\n",
            "\n",
            "Conclusion\n",
            "\n",
            "In conclusion, we have covered steps to deploy the model into an api service in your local computer.\n",
            "\n",
            "The next step is to deploy this in a cloud server as a micro service. In following blog’s I will cover the use of container service such as docker and deploy it in AWS.\n",
            "\n",
            "---\n",
            "\n",
            "A Recipe for using Open Source Machine Learning models\n",
            "\n",
            "Photo by Luca Bravo on Unsplash\n",
            "\n",
            "Machine learning continues to produce state of the art (SOTA) results for an increasing variety of tasks and more companies are looking to ML to solve their problems. With the incredibly rapid pace of machine learning research, many of these SOTA models come from academic and research institutions which open source these models. Often, using one of these open source models to bootstrap your machine learning efforts within your company can be much more effective than building a model from scratch.\n",
            "\n",
            "However, these models are often released by researchers whose focus isn’t necessarily to enable easy use and modification of their models (though there are many exceptions). Using these open source models for your tasks can be quite difficult.\n",
            "\n",
            "In this post, my goal is to provide a recipe you can follow to evaluate and use open source ML models to solve your own tasks. These are the steps I’ve used over and over again in my own work (as of this writing I have anaconda environments set up over 15 open source models). As my work is mostly using deep learning for vision and NLP, my focus here is specifically on using neural network-based models.\n",
            "\n",
            "Whether you’re trying to use machine learning to solve real problems within your company or experiment with some fun SOTA results at home, my hope is that after this post, you’ll have a path to take an open source model and modify and use it to address your own task with your own dataset.\n",
            "\n",
            "Step 1: Naming your task\n",
            "\n",
            "The first step is figuring out what your particular task is called in the research literature so you can successfully search for it. This can initially be quite frustrating. For example, finding all the instances of a dog in a picture would be an “object detection” task. But if you want to know exactly which pixels in the picture correspond to dogs that’s called “image segmentation.”\n",
            "\n",
            "There are a few ways you can try to figure this out. First, if you happen to know any ML researchers or practitioners, definitely start there. Another option is to ask in r/machinelearning or r/learnmachinelearning. If none of these pan out, the next step is to google to the best of your ability. As you land on research papers you’ll often see the name commonly associated with the task in the literature.\n",
            "\n",
            "Step 2: Finding papers and code\n",
            "\n",
            "Once you know what to search for, the next step is to find those open source models that best suit your task. There are a few resources that are helpful here:\n",
            "\n",
            "paperswithcode: A repository of papers and associated code, organized by task. This is a really good starting point, especially if it’s a well-known task.\n",
            "\n",
            "arxiv-sanity: Many open source models are associated with research papers. Most papers in machine learning are (fortunately!) openly published on arxiv. Searching arxiv for recent papers that solve for your task is another good place to start. Not all published papers here have code associated with them. If you find a paper you like, try searching for “<paper name> github” to see if the code has been released.\n",
            "\n",
            "Kaggle: If there happens to be a Kaggle competition with a task similar to yours, this can be a great way to get high quality, state of the art models. Pay particularly close attention to winner blogs for past competitions, these often have great explanations and code. The little tricks that were used to win the competition can often be really valuable for your task as well.\n",
            "\n",
            "Dataset benchmarks: If there’s a benchmark dataset that’s similar to the task you’re working on, the leaderboard for that benchmark is a quick way to find papers with demonstrably SOTA results.\n",
            "\n",
            "Google: For standard/common tasks like image segmentation, searching for “image segmentation github”, “image segmentation pytorch” or “image segmentation tensorflow” will give you a lot of results.\n",
            "\n",
            "Step 3: Read the papers\n",
            "\n",
            "This can be intimidating because academic papers can be pretty inaccessible, even to experienced software engineers. But if you focus on the abstract, introduction, related work, results, and delay a lot of the deep details/math for later readings, you’ll find you can get a lot out of the paper and a deeper understanding of the problem.\n",
            "\n",
            "Pay particularly close attention to the dataset(s) they use and the constraints of those datasets or their model. Often you’ll find that the constraints may not be applicable to you and are fundamental to the model design. For example, classification models for imagenet expect there to be one and only one salient object in an image. If your images have zero, one or more objects to identify, those models are probably not directly applicable. This isn’t something you want to find out after investing the time needed to bring up the model.\n",
            "\n",
            "Also, follow some of the references, especially those you see in multiple papers! You’ll regularly find that at least one of the references provides a very clear description of the problem and dramatically increases your understanding. Referenced papers may also end up being more useful and may have nicer code associated with them, so it’s worth doing a little digging here.\n",
            "\n",
            "Step 4: Make sure the code is usable\n",
            "\n",
            "Once you’ve found a paper with open source code, make sure it’s usable. Specifically:\n",
            "\n",
            "Check the license: While a lot of code is released under liberal open source licenses (MIT, BSD, Apache etc), some of it isn’t. You may find the model has a non-commercial use only license, or no license at all. Depending on your use case and company, the code may or may not be usable for you.\n",
            "\n",
            "Check the framework: If you’re working with a particular framework (eg. Tensorflow, Pytorch) check the framework the model is built in. Most of the time you’re stuck with what you get, but sometimes there’s a reimplementation of the model in your preferred framework. A quick Google to check for this (eg. “<paper name> pytorch”) can save you a lot of trouble.\n",
            "\n",
            "Check the language: Similarly, if the model is in Lua and you’re not a Lua developer, this can be really painful. See if there’s a reimplementation in the language of your choice (often Python, since in deep learning Python should be part of your repertoire), and if not you might be better off finding another model.\n",
            "\n",
            "Check the coding style: Researchers aren’t all software engineers so you can’t have as high a bar as for other open source projects, but if the code is a total mess you may want to look for another model.\n",
            "\n",
            "Step 5: Get the model running\n",
            "\n",
            "Results from NVIDIA’s StyleGAN trained on a custom furniture dataset\n",
            "\n",
            "Once you’ve found a model you think is a good fit, try to get the model running. The goal here is to run the training and inference loop for the model as-is, not to get it running on your specific dataset or to make any significant modifications. All you want to do is make sure that you have the right dependencies and that the model trains and runs as advertised. To that end:\n",
            "\n",
            "Create a conda environment for the model: You may be trying out multiple models, so create a conda environment (assuming Python) for each model (nvidia-docker is another option here, but personally I find it to be overkill).\n",
            "\n",
            "I’ll often set up my environments like so: conda create -n <name of the github repo> python=<same version of python used by the repo>\n",
            "\n",
            "A quick way to figure out which version of python the repo is using is to look at the print statements. If there are no parens, it’s python 2.7, otherwise 3.6 should work.\n",
            "\n",
            "Install the libraries: I highly recommend starting off by installing the exact same version of the framework that the original code used to start. If the model says it works with pytorch>0.4.0 , don’t assume it’ll work with pytorch 1.0. At this stage, you don’t want to be fixing those kinds of bugs, so start with pytorch=0.4.0 . You can install a particular version of a framework (eg. pytorch) with the command conda install pytorch=0.4.0 -c pytorch . A lot of code won’t have a requirements.txt file, so it may take some sleuthing and iterating to figure out all the libraries you need to install.\n",
            "\n",
            "Get the original dataset and run the scripts: At this point, you should be able to download the original dataset and run the testing and training script. You’ll probably have to fix some paths here and there and use the README and source to figure out the correct parameters. If a pre-trained model is available, start with the testing script and see if you’re getting similar results to the paper.\n",
            "\n",
            "Once you have the testing script running, try to get the training script up. You’ll probably have to work through various exceptions and make slight modifications to get it to work. Ultimately your goal with the training script is to see the loss decreasing with each epoch.\n",
            "\n",
            "If it’s straightforward (ie. only requires changing some command-line flags), at this point you might try running training script on your own dataset. Otherwise, we’ll do this in step 7.\n",
            "\n",
            "Step 6: Create your own testing notebook\n",
            "\n",
            "At this point, you’ve confirmed that the model works and you have the right environment set up to be able to use it. Now you can dig in and start really playing with it. At this point, I recommend creating a Jupyter notebook, copy-pasting in the testing script, and then modifying till you can use it with a single item of data. For example, if you’re using an object detection model that finds dogs in an image, you want a notebook where you can pass it a picture and have it output the bounding boxes of the dogs.\n",
            "\n",
            "The goal here is to get a feel for the inputs and outputs, how they must be formatted and how exactly the model works, without having to deal with the additional complexity of training or munging your own data into the right formats. I recommend doing this in a Jupyter notebook because I find that being able to see the outputs along each step is really helpful in figuring it out.\n",
            "\n",
            "Step 7: Create your own training notebook with your dataset\n",
            "\n",
            "Now that you have some familiarity with the model and data, it’s time to try to create a training notebook. Similar to step 6, I start by copying and pasting in the training script, separating it into multiple cells, and then modifying it to fit my needs.\n",
            "\n",
            "If you’re already feeling comfortable with the model, you may want to go directly to modifying the training notebook so it works with your dataset. This may involve writing dataloaders that output the same format as the existing dataloaders in the model (or simply modifying those dataloaders). If you’re not yet comfortable enough to do that, start by just getting the training script to work as-is in the notebook and removing code that you don’t think is useful. Then work on getting it to work with your dataset.\n",
            "\n",
            "Keep in mind the goal here isn’t to modify the model, even if it’s not quite solving the exact task you want yet. It’s just to get the model working with your dataset.\n",
            "\n",
            "Step 8: Start modifying the model to suit your task!\n",
            "\n",
            "By this point, you should have a notebook that can train the model (including outputting appropriate metrics/visualizations) and a notebook where you can test new models you create. Now is a good time to start to dig in and make modifications to the model (adding features, additional outputs, variations etc) that make it work for your task and/or dataset. Hopefully having the starting point of an existing state of the art model saved you a lot of time and provides better results than what you might get starting from scratch.\n",
            "\n",
            "There’s obviously a lot happening in this step and you’d use all your existing model building strategies. However, below are some pointers that may be helpful specifically when building off of an existing model.\n",
            "\n",
            "Modify the dataset before modifying the model: It’s often easier to munge your data into the format the model expects rather than modifying the model. It’s easier to isolate problems and you’re likely to introduce fewer bugs. It’s surprising how far you can sometimes push a model just by changing the data.\n",
            "\n",
            "Reuse the pre-trained model as much as possible: If your model changes aren’t drastic, try to reuse the pre-trained model parameters. You may get faster results and the benefits of transfer learning. Even if you expand the model, you can often load the pre-trained parameters into the rest of the model (eg. use strict=False when loading the model in pytorch).\n",
            "\n",
            "Make incremental changes and regularly check the performance: One benefit of using an existing model is you have an idea of the performance you started with. By making incremental changes and checking the performance after each one, you’ll immediately identify when you’ve made a mistake or are going down a bad path.\n",
            "\n",
            "Ask for help: If you’re totally stuck, try reaching out to the author and asking for some pointers. I’ve found they’re often willing to help, but remember they’re doing you a favor and please act accordingly.\n",
            "\n",
            "Automatically texturing a 3D model using neural renderer\n",
            "\n",
            "Step 9: Attribute and Contribute\n",
            "\n",
            "Depending on the license and how you’re distributing your model, you may be required to provide attribution to the developer of the original code. Even if it’s not required, it’s nice to do it anyway.\n",
            "\n",
            "And please contribute back if you can! If you come across bugs and fix them during your own development, submit a pull request. I’m sure well-written bug reports are welcome. Finally, if nothing else, sending a quick thank you to the author for their hard work is always appreciated.\n",
            "\n",
            "---\n",
            "\n",
            "In this article I’ll show you how to go from concept to deployment with a computer vision classification model. With fastai you can quickly build and train a state-of-the-art deep learning model. Then, Render makes hosting and deploying your app a breeze.\n",
            "\n",
            "We’ll go step-by-step as we build and deploy a model to classify skin lesion images. When finished, users will be able upload an image to a website and the model will classify their skin lesion.\n",
            "\n",
            "I plan to write a similar article on NLP soon — follow me to make sure you don’t miss it! 😄\n",
            "\n",
            "Disclaimers:\n",
            "\n",
            "Health/legal: This project is for demonstration purposes only. If you think you may have a skin problem, go see a health care professional. I’m not a health care professional.\n",
            "\n",
            "Technical: This example isn’t intended for a large-scale website with millions of hits. If you have that issue — well, that’s a good problem to have. 😀 This setup might work. Render uses Docker and Kubernetes behind the scenes to make scaling as close to painless as possible.\n",
            "\n",
            "Alright — back to the action!\n",
            "\n",
            "Training set images\n",
            "\n",
            "Skin cancers are the most common group of cancers. A number of apps have been developed to classify skin lesions. By uploading a picture of your suspicious skin spot you can see if you should talk to a dermatologist.\n",
            "\n",
            "Here’s the plan:\n",
            "\n",
            "Find the data. Build and train the model. Deploy the model. Improve model performance.\n",
            "\n",
            "Let’s flesh this plan out a bit. 😉\n",
            "\n",
            "Steps\n",
            "\n",
            "---\n",
            "\n",
            "Section 2\n",
            "\n",
            "Architecture Diagram\n",
            "\n",
            "Install the Gunicorn Python package:\n",
            "\n",
            "$ pip install gunicorn\n",
            "\n",
            "Configure gunicorn.conf file to configure Gunicorn web server.\n",
            "\n",
            "We have configured Gunicorn web server to listen on port 5000 and run the main.py file from the app directory.\n",
            "\n",
            "Configure supervisord as monitoring process:\n",
            "\n",
            "Supervisord allows a user to monitor and control several processes on UNIX-like operating systems. Supervisor will look after the Gunicorn process and make sure that they are restarted if anything goes wrong, or to ensure the processes are started at boot time.\n",
            "\n",
            "It will run NGINX reverse proxy server and keep a monitor on it. If anything fails it will automatically restart the server and run NGINX command.\n",
            "\n",
            "Set up NGINX server:\n",
            "\n",
            "Open up a server block and set NGINX to listen on the default port 80 . Users can set the server name to handle the request.\n",
            "\n",
            "Create Dockerfile:\n",
            "\n",
            "I have divided Dockerfile into 5 sections. Let’s go briefly into each section:\n",
            "\n",
            "Create an Ubuntu environment. Install Ubuntu and update the necessary package. Install python, pip, virtual environment, NGINX, Gunicorn, and supervisor. Set up a flask application. Make a directory “/deploy.app”. Copy all files from the “app” folder to “/deploy/app” folder. Install all the required python packages. Set up NGINX. Remove default NGINX configuration. Copy flask configuration to NGINX configuration. Create a symbolic link for NGINX flask configuration. Set up supervisord. Make a directory “/var/log/supervisor”. Copy Gunicorn and supervisord configuration to the newly-created directory. Start supervisord process for monitoring.\n",
            "\n",
            "Build and run docker image to test the production-ready ML web API:\n",
            "\n",
            "I have created a makefile to run all the commands.\n",
            "\n",
            "The below Makefile performs two operations:\n",
            "\n",
            "Create a docker image Run docker image on port 80\n",
            "\n",
            "Run Makefile:\n",
            "\n",
            "deploy-local: #build docker image\n",
            "\n",
            "docker build -t gcr.io/${project_id}/${image_name}:${version} . #run docker image\n",
            "\n",
            "docker run -d --name ${image_name} -p $(port):$(port) gcr.io/${project_id}/${image_name}:${version}\n",
            "\n",
            "Test production-ready web API on development machine using Postman on URL: http://127.0.0.1/predict\n",
            "\n",
            "In the above section, we have learned how to to build and run an ML web API docker image on the development machine.\n"
          ]
        }
      ],
      "source": [
        "print(Values[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3prexz41IA_6",
        "outputId": "b6ea0794-34f7-4676-92bd-312e5b2c68b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Implementing Random Forest in R. A Practical Application of Random…\n",
            "The Basics: Decision Tree Classifiers\n",
            "Is Random Forest better than Logistic Regression? (a comparison)\n",
            "The Complete Guide to Decision Trees\n"
          ]
        }
      ],
      "source": [
        "print(Keys[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pbq1L6quICuh",
        "outputId": "5d9db18b-d353-4752-90b7-c6ceec7d9ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Implementing Random Forest in R\n",
            "\n",
            "Photo by Rural Explorer on Unsplash\n",
            "\n",
            "What is Random Forest (RF)?\n",
            "\n",
            "In order to understand RF, we need to first understand about decision trees. Rajesh S. Brid wrote a detailed article about decision trees. We will not go too much in details about the definition of decision trees since that is not the purpose of this article. I just want to quickly summarise a few points. A decision tree is series of Yes/No questions. For each level of the tree, if your answer is Yes, you fall into a category, otherwise, you will fall into another category. You will answer this series of Yes/No questions until you reach the final category. You will be classified into that group.\n",
            "\n",
            "Taken from here\n",
            "\n",
            "Trees work well with the data we use to train, but they are not performing well when it comes to new data samples. Fortunately, we have Random Forest, which is a combination of many decision trees with flexibility, hence resulting in an improvement in accuracy.\n",
            "\n",
            "Here I will not go too much into details about RF because there are various sources outside we can get to understand what is the mathematics behind it. Here is one of them.\n",
            "\n",
            "This article is more about practical application of RF in classifying cancer patients, so I will jump straight into the coding part. Now let’s open Rstudio and get our hands dirty :)\n",
            "\n",
            "Implementing RF in R\n",
            "\n",
            "First of all, we need to load the following packages. If you cannot load them, chances are you have not installed them yet. So please do so first before loading the following packages.\n",
            "\n",
            "I read the data directly from the web link and name the dataset as Wisconsin. Let’s inspect the data a little bit\n",
            "\n",
            "---\n",
            "\n",
            "DATA SCIENCE FROM THE GROUND UP\n",
            "\n",
            "The Basics: Decision Tree Classifiers\n",
            "\n",
            "Decision trees are a conceptually simple and explicable style of model, though the technical implementations do involve a bit more calculation that is worth understanding. Still, the intuition behind a decision tree should be easy to understand. Indeed, decision trees are in a way quite similar to how people actually make choices in the real world.\n",
            "\n",
            "What\n",
            "\n",
            "When confronted with a choice to make, real people might think in a series of cascading decisions. If I need to choose what to wear in the morning, I don’t pick an outfit randomly out of the closet, I first breakdown my options. For starters, I probably already have heavy winter clothes and lighter summer clothes separated. I then might check the weather, further narrowing down the range of outfits — if it’s raining I might want to wear my boots, say. Finally, I may consider what plans I have today and if I need to dress up for any reason. You can think of my decision process as starting with all the options in my closet, and then progressively narrowing down my choices until I’m picking from a much smaller set of options.\n",
            "\n",
            "Perhaps we can teach the computer to make decisions or predictions in a similar way. One aspect is that the decision making process is rules based, which makes it easy to implement on a computer, assuming we know what the rules are. The task therefore is finding what the rules are.\n",
            "\n",
            "Consider a simple classification problem. Here are points scattered among three categories:\n",
            "\n",
            "A simple three category classification problem\n",
            "\n",
            "If presented with a new point somewhere on the grid, how would you predict which category it belonged to? You might start by noticing, say, that all the points in the upper portion of the grid are orange. So, let’s start by simply drawing a horizontal line at the bottom of this ‘orange territory’. If the new point we’re trying to predict falls above this line, we’ll guess it belongs to the orange category:\n",
            "\n",
            "---\n",
            "\n",
            "Introduction:\n",
            "\n",
            "Random Forests are another way to extract information from a set of data. The appeals of this type of model are:\n",
            "\n",
            "It emphasizes feature selection — weighs certain features as more important than others.\n",
            "\n",
            "It does not assume that the model has a linear relationship — like regression models do.\n",
            "\n",
            "It utilizes ensemble learning. If we were to use just 1 decision tree, we wouldn’t be using ensemble learning. A random forest takes random samples, forms many decision trees, and then averages out the leaf nodes to get a clearer model.\n",
            "\n",
            "In this analysis we will classify the data with random forest, compare the results with logistic regression, and discuss the differences. Take a look at the previous logistic regression analysis to see what we‘ll be comparing it to.\n",
            "\n",
            "Table of Contents:\n",
            "\n",
            "1. Data Understanding (Summary)\n",
            "\n",
            "2. Data Exploration/Visualization(Summary)\n",
            "\n",
            "4. Building the Model\n",
            "\n",
            "5. Testing the Model\n",
            "\n",
            "6. Conclusions\n",
            "\n",
            "Data Background:\n",
            "\n",
            "We have a sample of 255 patients and would like to measure the relationship between 4 proteins levels and cancer growth.\n",
            "\n",
            "We know:\n",
            "\n",
            "The concentration of each protein measured per patient.\n",
            "\n",
            "Whether or not each patient has been diagnosed with cancer (0 = no cancer; 1= cancer).\n",
            "\n",
            "Our goal is: To predict whether future patients have cancer by extracting information from the relationship between protein levels and cancer in our sample.\n",
            "\n",
            "The 4 proteins we’ll be looking at:\n",
            "\n",
            "Alpha-fetoprotein (AFP)\n",
            "\n",
            "Carcinoembryonic antigen (CEA)\n",
            "\n",
            "Cancer Antigen 125 (CA125)\n",
            "\n",
            "---\n",
            "\n",
            "The Complete Guide to Decision Trees\n",
            "\n",
            "A complete introduction to decision trees, how to use them for regression and classification, and how to implement the algorithm in a project setting Marco Peixeiro · Follow Published in Towards Data Science · 9 min read · Jul 25, 2019 -- 2 Share\n",
            "\n",
            "They are… don’t even try something else\n",
            "\n",
            "Tree-based methods can be used for regression or classification. They involve segmenting the prediction space into a number of simple regions. The set of splitting rules can be summarized in a tree, hence the name decision tree methods.\n",
            "\n",
            "A single decision tree is often not as performant as linear regression, logistic regression, LDA, etc. However, by introducing bagging, random forests, and boosting, it can result in dramatic improvements in prediction accuracy at the expense of some loss in interpretation.\n",
            "\n",
            "In this post, we introduce everything you need to know about decision trees, bagging, random forests, and boosting. It will be a long read, but it will be worth it!\n"
          ]
        }
      ],
      "source": [
        "print(Values[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjuw0knikv0L"
      },
      "source": [
        "# Langchain & LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gv3jjf5UkzXh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import httpx\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY,\n",
        "                http_client=httpx.Client(proxy=OPENAI_PROXY,\n",
        "                                          transport=httpx.HTTPTransport(local_address=\"0.0.0.0\")),\n",
        "                )\n",
        "\n",
        "\n",
        "def LLM_request(query: str, context: str) -> dict:\n",
        "\n",
        "    system_prompt = f\"\"\"DOCUMENT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {query}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    Answer the users QUESTION using the DOCUMENT text above.\n",
        "    Keep your answer ground in the facts of the DOCUMENT.\n",
        "    If the DOCUMENT doesn’t contain the facts to answer the QUESTION return Нет данных.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()   # record the time before the request is sent\n",
        "\n",
        "    LLM = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt}],\n",
        "            model=\"gpt-4o\",\n",
        "            temperature=0.2,\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time    # calculate the time it took to receive the response\n",
        "\n",
        "    return {\n",
        "            \"content\": LLM.choices[0].message.content,\n",
        "            \"prompt_tokens\": LLM.usage.prompt_tokens,\n",
        "            \"completion_tokens\": LLM.usage.completion_tokens,\n",
        "            \"total_tokens\": LLM.usage.total_tokens,\n",
        "            \"elapsed_time\": elapsed_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vvaniJDql0pq"
      },
      "outputs": [],
      "source": [
        "LLM_Results = [LLM_request(Questions[i], Values[i]) for i in range(len(Questions))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeSQzTpWJKlA",
        "outputId": "0ed71a0a-9f23-4693-dbfb-fe3d222493ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Convolutional Neural Networks (CNNs) can be applied in various domains beyond their initial use in image processing. Some common applications include:\n",
            "\n",
            "1. **Image Processing**: CNNs are widely used for image classification, segmentation, and enhancement tasks. Examples include identifying satellite images that contain roads or classifying handwritten letters and digits.\n",
            "\n",
            "2. **Natural Language Processing (NLP)**: CNNs are used for understanding and processing language data, although Recurrent Neural Networks (RNNs) are often preferred for certain NLP tasks.\n",
            "\n",
            "3. **Speech Recognition**: CNNs can be used to process audio data for recognizing speech patterns.\n",
            "\n",
            "4. **Population Genetics**: CNNs are applied in genetic inference tasks such as performing selective sweeps, finding gene flow, and inferring population size changes.\n",
            "\n",
            "5. **Astrophysics**: CNNs help interpret radio telescope data to predict visual images representing the data.\n",
            "\n",
            "6. **Voice Synthesis**: Deepmind’s WaveNet, a CNN model, is used for generating synthesized voice, forming the basis for Google’s Assistant’s voice synthesizer.\n",
            "\n",
            "7. **Single Cell Genomics**: Researchers use CNNs for generative models in disease identification within genomics.\n",
            "\n",
            "These examples illustrate the diverse and expanding applications of CNNs across different fields.\n"
          ]
        }
      ],
      "source": [
        "print(LLM_Results[0]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RI59W3sJOS4",
        "outputId": "cd12a701-5660-46cc-a7e5-8a009528a70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinforcement Learning (RL) is a framework in which an agent learns to interact with its environment by taking actions and receiving feedback in the form of rewards. The goal of the agent is to maximize the expected cumulative reward over time. In RL, the agent does not receive explicit instructions on how to perform tasks but instead learns through trial and error, guided by the rewards it receives. The environment provides the agent with a state, and the agent must choose an action that leads to a new state and a reward. RL problems can be categorized into episodic tasks, which have a defined start and end, and continuing tasks, which go on indefinitely. The learning process often involves concepts such as the Markov Decision Process (MDP), which provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.\n"
          ]
        }
      ],
      "source": [
        "print(LLM_Results[1]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6LW-iyuJRPB",
        "outputId": "671c652c-e3d7-4ea6-a26d-7d1f20636173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To deploy a machine learning model, you can follow these steps:\n",
            "\n",
            "1. **Convert the Model**: Save your trained model in a suitable format. If it's a model built with sklearn, save it as a `.pkl` file. If it's a deep learning model, save it as an `.hdf5` file.\n",
            "\n",
            "2. **Implement an API**: Use Flask to create an API for your model. Load the saved model and create a Flask application object. Develop a test API function to ensure the API's health and a \"POST\" request API for processing requests to your model.\n",
            "\n",
            "3. **Run the API Locally**: Set up the Flask app to run on your local machine. You can test the API using a browser or tools like Postman.\n",
            "\n",
            "4. **Deploy on a Cloud Server**: Use container services like Docker to package your application. Set up a Dockerfile to create an environment with all necessary dependencies and configurations. Use a web server like Gunicorn and a reverse proxy like NGINX for handling requests.\n",
            "\n",
            "5. **Monitor and Manage**: Use tools like Supervisord to monitor your processes and ensure they restart if they fail.\n",
            "\n",
            "6. **Test the Deployment**: Once deployed, test your API to ensure it works as expected in the production environment.\n",
            "\n",
            "These steps provide a general framework for deploying a machine learning model from development to production.\n"
          ]
        }
      ],
      "source": [
        "print(LLM_Results[2]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcxH_rMtJSpQ",
        "outputId": "9cfd2fbf-4731-468e-9ffb-1922d62fb25c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To implement a random forest algorithm, you can follow these steps as outlined in the DOCUMENT:\n",
            "\n",
            "1. **Understand Decision Trees**: Before implementing a random forest, it's important to understand decision trees, as a random forest is essentially a collection of decision trees. Decision trees involve making a series of Yes/No decisions to classify data into categories.\n",
            "\n",
            "2. **Use Ensemble Learning**: Random forests utilize ensemble learning by creating multiple decision trees and averaging their results to improve accuracy. This method emphasizes feature selection and does not assume a linear relationship in the data.\n",
            "\n",
            "3. **Load Necessary Packages**: If you are using R, you need to load specific packages to work with random forests. If these packages are not already installed, you will need to install them first.\n",
            "\n",
            "4. **Read and Inspect Data**: Load your dataset into R for analysis. For example, in the DOCUMENT, the dataset named \"Wisconsin\" is read directly from a web link.\n",
            "\n",
            "5. **Build and Test the Model**: Use the random forest algorithm to classify your data. In the example provided, the goal is to classify cancer patients based on protein levels.\n",
            "\n",
            "6. **Compare with Other Models**: After building the random forest model, you can compare its performance with other models like logistic regression to understand the differences and effectiveness.\n",
            "\n",
            "These steps provide a practical approach to implementing a random forest algorithm, particularly in R, as described in the DOCUMENT.\n"
          ]
        }
      ],
      "source": [
        "print(LLM_Results[3]['content'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
